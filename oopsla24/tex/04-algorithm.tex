\label{sec:algorithm}

In this section, we present the algorithmization of the declarative system described above.
The algorithmic system follows the structure of the declarative specification closely.
First, it is also given by a set of inference rules, which, however,
must be mode-correct (\cite{dunfield2020:bidirectional}), \ie
the output of each rule is always uniquely defined by its input.
And second, most of the declarative rules 
(except for the rules \ruleref{\ottdruleDTPEquivLabel} and \ruleref{\ottdruleDTNEquivLabel})
have a unique algorithmic counterpart, 
which simplifies reasoning about the algorithm and its correctness proofs.


\subsection{Algorithmic Syntax}
\label{sec:algo-syntax}

First, let us discuss the syntax of the algorithmic system. 

\begin{figure}[h]
  \begin{multicols}{2}
      Negative Algorithmic Variables\\
      $[[α̂⁻]]$, $[[β̂⁻]]$, $[[γ̂⁻]]$, \dots\\
      \columnbreak
      Positive Algorithmic Variables\\
      $[[α̂⁺]]$, $[[β̂⁺]]$, $[[γ̂⁺]]$, \dots\\
  \end{multicols}
  \hfill\\
  \begin{multicols}{2}
      \ottuNShort
      \columnbreak
      \ottuPShort
  \end{multicols}
  \hfill\\
  Algorithmic Type Context\\
   $[[Ξ]] = \{[[α1̂±]], \dots, [[αn̂±]]\}$ where $[[α1̂±]], \dots, [[αn̂±]]$ are pairwise distinct \\
  \hfill\\
  Constraint Type Context\\
   $[[Θ]] ::= \{[[ α1̂±[Γ1] ]], \dots, [[ αn̂±[Γn] ]]\}$ where $[[α1̂±]], \dots, [[αn̂±]]$ are pairwise distinct \\
  \caption{Algorithmic Syntax}
  \label{fig:algo-syntax}
\end{figure}

\paragraph{Algorithmic Variables}
To design a mode-correct inference system, we slightly modify the language we operate on.
The entities (terms, types, contexts) that the algorithm manipulates we call \emph{algorithmic}. 
They extend the previously defined declarative terms and types by adding 
\emph{algorithmic type variables} (\aka unification variables). 
The algorithmic variables represent unknown types, 
which cannot be inferred immediately but are promised to be instantiated
as the algorithm proceeds.

We denote algorithmic variables as $[[α̂⁺]]$, $[[β̂⁻]]$, \dots to distinguish them from
normal variables $[[α⁺]]$, $[[β⁻]]$.
In a few places, we replace the quantified variables $[[pas]]$ 
with their algorithmic counterpart $[[puas]]$.
The procedure of replacing declarative variables with algorithmic ones we call 
\emph{algorithmization} and denote as $[[ nuas/nas ]]$ and $[[ puas/pas ]]$.

\paragraph{Algorithmic Types}
The syntax of algorithmic types extends the declarative syntax by adding
algorithmic variables as new terminals. We add positive algorithmic variables $[[α̂⁺]]$ 
to the syntax of positive types, and negative algorithmic variables $[[α̂⁻]]$ to the 
syntax of negative types. All the constructors of the system can be applied 
to \emph{algorithmic} types, however, algorithmic variables cannot be abstracted by the
quantifiers $[[∀]]$ and $[[∃]]$.

\paragraph{Algorithmic Contexts and Well-formedness}
To specify when algorithmic types are well-formed, 
we define algorithmic contexts $[[Ξ]]$ as sets of algorithmic variables.
Then $[[Γ ; Ξ ⊢ uP]]$ and $[[Γ ; Ξ ⊢ uN]]$ represent the well-formedness
judgment of algorithmic terms defined as expected. 
Informally, they check that all free declarative variables are in $[[Γ]]$, 
and all free algorithmic variables are in $[[Ξ]]$.
Most of the rules are inherited from the well-formedness of 
\emph{declarative} types: the declarative variables are
checked to belong to the context $[[Γ]]$, the quantifiers extend the context,
type constructors are well-formed congruently.
As we extend the syntax with algorithmic variables, we also add 
two base-case rules for them:
 \ruleref{\ottdruleWFATPUVarLabel} and \ruleref{\ottdruleWFATNUVarLabel} 
 (see \cref{fig:algo-wf}).

\begin{figure}[h]
\begin{multicols}{2}
  $\vcenter{\hbox{\vdots}}$\\
  \ottusedrule{\ottdruleWFATPUVarLabeled{}}
  \columnbreak\\
  $\vcenter{\hbox{\vdots}}$\\
  \ottusedrule{\ottdruleWFATNUVarLabeled{}}
\end{multicols}
\caption{Well-formedness of Algorithmic Types}
\label{fig:algo-wf}
\end{figure}

\paragraph{Algorithmic Normalization}
Similarly to well-formedness, the normalization of algorithmic types is defined
by extending the declarative definition with the algorithmic variables.
To the rules repeating the declarative normalization, we add
rules saying that normalization is trivial on algorithmic variables (see \cref{fig:algo-nf}).

\begin{figure}[h]
\begin{multicols}{2}
  $\vcenter{\hbox{\vdots}}$\\
  \ottusedrule{\ottdruleNrmPUVar{}}
  \columnbreak\\
  $\vcenter{\hbox{\vdots}}$\\
  \ottusedrule{\ottdruleNrmNUVar{}}
\end{multicols}
\label{fig:algo-nf}
\caption{Normalization of Algorithmic Types}
\end{figure}

\subsection{Type Constraints}
As the algorithm proceeds, it accumulates the information 
about the algorithmic type variables in the form of \emph{constraints}.
In our system, the constraints can be of two kinds: 
\emph{subtyping constraints} and \emph{unification constraints}.
The subtyping constraint can only have a positive shape $[[α̂⁺ :≥ iP]]$, \ie it 
restricts a positive algorithmic variable to be a 
supertype of a certain declarative type---this is one of the invariants that we 
preserve in the algorithm.
The unification constraint can have either a positive form $[[α̂⁺ :≈ iP]]$ or
a negative form $[[α̂⁻ :≈ iN]]$, however, the right-hand side of the constraint
cannot contain algorithmic type variables.
The set of constraints is denoted as $[[SC]]$. 
We assume that each algorithmic variable can be restricted by at most one constraint.

We separately define $[[UC]]$ as a set consisting of unification constraints only.
This is done to simplify the representation of the algorithm. 
The unification algorithm, which we use as a subroutine of the subtyping algorithm,
can only produce unification constraints.
A set of unification constraints can be resolved in a simpler way than
a general constraint set. This way, 
the separation of the unification constraint resolution into a separate
procedure allows us to better decompose the structure of the algorithm,
and thus, simplify the inductive proofs.


  \begin{figure}[h]
    \begin{multicols}{2}
      \ottgrammartabular { 
      \ottscE 
      \ottinterrule
      \ottruleheadOneLine
        {[[SC]]}{::=} {\ottcom{Constraint Set}}
        {\{[[scE1]], \dots, [[scEn]]\}}\ottprodnewline
      }

    \columnbreak

      \ottgrammartabular { 
      \ottucE 
      \ottprodnewline
      \ottinterrule
      \ottruleheadOneLine
        {[[UC]]}{::=} {\ottcom{Unification Constraint Set}}
        {\{[[ucE1]], \dots, [[ucEn]]\}}\ottprodnewline
      }
    \end{multicols}

    \label{fig:syntax-e-sc}
    \caption{Constraint Entries and Sets}
  \end{figure}

\paragraph{Constraint Contexts}
When one instantiates an algorithmic variable, 
they may only use type variables available in its scope.
As such, each algorithmic variable must remember the context at the moment when 
it was introduced. In our algorithm, this information is represented by
a \emph{constraint context} $[[Θ]]$---a set of pairs associating 
algorithmic variables and declarative contexts.

\paragraph{Auxiliary Functions}
We define $[[dom(SC)]]$---a domain of a constraint set $[[SC]]$ as a set of algorithmic variables
that it restricts. Similarly, we define $[[dom(Θ)]]$---a domain of constraint context
as a set of algorithmic variables that $[[Θ]]$ associates with their contexts.
We write $[[Θ(α̂±)]]$ to denote the context associated with $[[α̂±]]$ in $[[Θ]]$.



%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\subsection{Subtyping Algorithm}
\label{sec:subtyping-algorithm}
  
  For convenience and scalability, 
  we decompose the subtyping algorithm 
  into several procedures. \Cref{fig:alg-subtyping-graph}
  shows these procedures and the dependencies between them:
  arrows denote the invocation of one procedure from another.
  The label <<$\ottkw{nf}$>> annotating arrows means that the calling
  procedure normalizes the input before passing it to the callee.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    [>={Stealth[scale=2]},node distance=2.4cm,every node/.style={draw,rectangle},every text node part/.style={align=center}]


    % Define nodes
    \node[] (1) {Negative Subtyping\\$[[Γ ; Θ ⊨ uN ≤ iM ⫤ SC]]$\\(\cref{fig:alg-subtyping})};
    \node[below of=1] (3) {Positive Subtyping\\$[[Γ ; Θ ⊨ uP ≥ iQ ⫤ SC]]$\\(\cref{fig:alg-subtyping})};
    \node[left=1.5cm of 3] (2) {Constraint Merge\\$[[Θ ⊢ SC1 & SC2 = SC3]]$\\(\cref{sec:constraint-merge})};
    \node[right=1.5cm of 3] (5) {Unification\\ $[[Γ ; Θ ⊨ uN ≈u iM ⫤ UC]]$\\ $[[Γ ; Θ ⊨ uP ≈u iQ ⫤ UC]]$\\(\cref{sec:unification})};
    \node[below of=3] (4) {Upgrade\\$[[upgrade Γ ⊢ iP to Δ = iQ]]$\\(\cref{sec:lub})};
    \node[below of=2] (6) {Least Upper Bound\\$[[Γ ⊨ iP1 ∨ iP2 = iQ]]$\\(\cref{sec:lub})};
    \node[below of=6] (7) {Anti-Unification\\$[[Γ ⊨ iP1 ≈au iP2 ⫤ ( Ξ , uQ , aus1 , aus2 )]]$\\$[[Γ ⊨ iN1 ≈au iN2 ⫤ ( Ξ , uM , aus1 , aus2 )]]$\\(\cref{sec:antiunification})};
    \node[below of=5] (8) {Unification Constraint Merge\\$[[Θ ⊢ UC1 & UC2 = UC3]]$\\(\cref{sec:constraint-merge})};

    % Define edges
    \draw[->] (1) to (2);
    \draw[->] (1) to (3);
    \draw[->] (1) to node[above, draw=none]{$\ottkw{nf}$} (5);
    \draw[->] (2) to (3);
    \draw[->] (2) to (6);
    \draw[->] (3) to (4);
    \draw[->] (3) to node[above, draw=none]{$\ottkw{nf}$} (5);
    \draw[->] (4) to (6);
    \draw[->] (5) to (8);
    \draw[->] (6) to node[left, draw=none]{$\ottkw{nf}$} (7);
  \end{tikzpicture}  
  \caption{Dependency graph of the subtyping algorithm}
  \label{fig:alg-subtyping-graph}
\end{figure}

In the remainder of this section, we will delve into each of these procedures in
detail, following the top-down order of the dependency graph. First, 
we present the subtyping algorithm itself.

As an input, the subtyping algorithm takes
a type context $[[Γ]]$, a constraint context $[[Θ]]$,
and two types of the corresponding polarity:
$[[uN]]$ and  $[[iM]]$ for the negative subtyping, and
$[[uP]]$ and  $[[iQ]]$ for the positive subtyping.
We assume the second type ($[[iM]]$ and $[[iQ]]$) to be 
declarative (with no algorithmic variables) and well-formed in $[[Γ]]$,
but the first type ($[[uN]]$ and $[[uP]]$) may contain algorithmic variables,
whose instantiation contexts are specified by $[[Θ]]$.

Notice that the shape of the input types uniquely determines the
applied subtyping rule.  If the subtyping is successful, it returns
a set of constraints $[[SC]]$ restricting the algorithmic 
variables of the first type. If the subtyping does not hold, 
there will be no inference tree with such inputs. 

\begin{figure}[h]
  \hfill\\
  \begin{multicols}{2}
    \ottdefnANsubLabeled{}
    \columnbreak\\
    \ottdefnAPsupLabeled{}
  \end{multicols}
  \caption{Subtyping Algorithm}
  \label{fig:alg-subtyping}
\end{figure}

The rules of the subtyping algorithm bijectively correspond to the rules of the declarative
system. Let us discuss them in detail.

\paragraph{Variables} Rules \ruleref{\ottdruleANVarLabel} and \ruleref{\ottdruleAPVarLabel}
say that if both of the input types are equal declarative variables,
they are subtypes of each other, with no constraints (as there are no algorithmic variables).

\paragraph{Shifts} Rules \ruleref{\ottdruleAShiftDLabel} and
\ruleref{\ottdruleAShiftULabel} cover the downshift and the upshift cases,
respectively. If the input types are constructed by shifts, then the subtyping
can only hold if they are equivalent. This way, the algorithm must find the
instantiations of the algorithmic variables on the left-hand side
such that these instantiations make the left-hand side and the ride-hand side
equivalent. For this purpose, the algorithm invokes the
unification procedure (\cref{sec:unification}) preceded by the normalization of the input types.
It returns the resulting constraints given by the unification algorithm. 

\paragraph{Quantifiers}  
Rules \ruleref{\ottdruleAForallLabel} and 
\ruleref{\ottdruleAExistsLabel} are symmetric. 
Declaratively, the quantified variables on the left-hand side must 
be instantiated with types, which are not known beforehand.
We address this problem by algorithmization 
of the quantified variables (see \cref{sec:algo-syntax}).
The rule introduces fresh algorithmic variables
$[[puas]]$ or $[[nuas]]$,
puts them into the constraint context $[[Θ]]$
(specifying that they must be instantiated in the extended context
$[[Γ, pbs]]$ or $[[Γ, nbs]]$) and substitute the quantified variables
for them in the input type. 

After algorithmization of the quantified variables, 
the algorithm proceeds with the recursive call, returning
constraints $[[SC]]$. As the output, the algorithm removes the freshly
introduced algorithmic variables from the constraint context, This operation is
sound: it is guaranteed that $[[SC]]$ always has a solution, but the specific
instantiation of the freshly introduced algorithmic variables is not important,
as they do not occur in the input types.

\paragraph{Functions}
To infer the subtyping of the function types, the algorithm
makes two calls: 
\begin{enumerate*}
  \item[(i)] a recursive call ensuring the subtyping of the result types, and
  \item[(ii)] a call to positive subtyping (or rather super-typing) on the argument types.
\end{enumerate*}
The resulting constraints are merged
(using a special procedure defined in \cref{sec:constraint-merge})
and returned as the output.

\paragraph{Algorithmic Variable}
If one of the sides of the subtyping is a unification variable,
the algorithm must create a new constraint. 
Because the right-hand side of the subtyping is always declarative,
it is only the left-hand side that can be a unification variable.
Moreover, another invariant we preserve prevents the negative
algorithmic variables from occurring in types during the
 negative subtyping algorithm. It means
that the only possible form of the subtyping here is $[[α̂⁺]] [[≥]] [[iP]]$,
which is covered by \ruleref{\ottdruleAPUVarLabel}.

The potential problem here is that the type $[[iP]]$
might be not well-formed in the context required for $[[α̂⁺]]$ by $[[Θ]]$, 
because this context might be smaller than the current context $[[Γ]]$.
As we wish the resulting constraint set to be sound \wrt $[[Θ]]$,
we cannot simply put $[[α̂⁺ :≥ iP]]$ into the output. 
Prior to that, we update the type $[[iP]]$ to its lowest supertype $[[iQ]]$
well-formed in $[[Θ(α̂⁺)]]$. It is done by the \emph{upgrade} procedure,
which we discuss in detail in \cref{sec:lub}.

\vspace{\baselineskip}
% \indent
To summarize, the subtyping algorithm uses the following additional subroutines:
\begin{enumerate*}[noitemsep]
  \item[(i)] rules \ruleref{\ottdruleAShiftDLabel} and
    \ruleref{\ottdruleAShiftULabel} invoke the \emph{unification} algorithm
    to equate the input types;
  \item[(ii)] rule \ruleref{\ottdruleAArrowLabel} \emph{merges} the constraints
    produced by the recursive calls on the result and the argument types; and
  \item[(iii)] rule \ruleref{\ottdruleAPUVarLabel} \emph{upgrades} the input type
    to its least supertype well-formed in the context required by the
    algorithmic variable.
\end{enumerate*}
The following sections discuss these additional procedures in detail.

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\subsection{Unification}
\label{sec:unification}

As an input the unification context 
takes a type context $[[Γ]]$, a constraint context $[[Θ]]$,
and two types of the required polarity:
$[[uN]]$ and  $[[iM]]$ for the negative unification, and
$[[uP]]$ and  $[[iQ]]$ for the positive unification.
It is assumed that only the left-hand side type may contain algorithmic variables,
this way, the left-hand side is well-formed as an algorithmic type in $[[Γ]]$ and 
$[[Θ]]$, whereas the right-hand side is well-formed declaratively in $[[Γ]]$.

Since only the left-hand side may contain algorithmic variables,
that the unification instantiates, we could have called this procedure \emph{matching}.
However, in \cref{sec:weakening-invariant}, we will discuss several modifications of the 
type system, where this invariant is not preserved, and therefore, this procedure
requires general first-order pattern unification \cite{miller1991:unification}.

As the output, the unification algorithm returns the weakest
set of unification constraints $[[UC]]$ such that any instantiation 
satisfying these constraints unifies the input types.

\begin{figure}[h]
  \hfill
  \begin{multicols}{2}
  \ottdefnUNUnifLabeled{}
  \columnbreak\\
  \ottdefnUPUnifLabeled{}
  \end{multicols}
  \caption{Unification Algorithm}
  \label{fig:unification}
\end{figure}

The algorithm works as one might expect:
if both sides are formed by constructors, 
it is required that the constructors are the same, and the
types unify recursively. If one of the sides
is a unification variable (in our case it can only be the left-hand side),
we create a new unification constraint restricting it to be equal to the other side.
Let us discuss the rules that implement this strategy. 

\paragraph{Variables}
  The variable rules \ruleref{\ottdruleUNVarLabel} and \ruleref{\ottdruleUPVarLabel}
  are trivial: as the input types do not have algorithmic variables, and are already equal, 
  the unification returns no constraints.

\paragraph{Shifts}
  The shift rules \ruleref{\ottdruleUShiftDLabel} and \ruleref{\ottdruleUShiftULabel}
  require the input types to be formed by the same shift constructor. 
  They remove this constructor, unify the types recursively, and return the resulting
  set of constraints.

\paragraph{Quantifiers} 
  Similarly, the quantifier rules \ruleref{\ottdruleUForallLabel} and \ruleref{\ottdruleUExistsLabel}
  require the quantifier variables on the left-hand side and the right-hand side to be the same.
  This requirement is complete because we assume the input types of the unification 
  to be normalized, and thus, the equivalence implies alpha-equivalence. 
  In the implementation of this rule, an alpha-renaming might be needed to ensure 
  that the quantified variables are the same, however, we omit it for brevity.

\paragraph{Functions}
  Rule \ruleref{\ottdruleUArrowLabel} unifies two functional types. 
  First, it unifies the argument types and their result types recursively.
  Then it merges the resulting constraints using the procedure described in \cref{sec:constraint-merge}.

  Notice that the resulting constraints can only have \emph{unification} entries.
  It means that they can be merged in a simpler way than general constraints.
  In particular, the merging procedure does not call any of the subtyping subroutines,
  but rather simply checks the matching constraint entries for equality.

\paragraph{Algorithmic Variable}
  Finally, if the left-hand side of the unification is an algorithmic variable,
  \ruleref{\ottdruleUNVarLabel} or \ruleref{\ottdruleUPVarLabel} is applied. 
  It simply checks that the right-hand side type is well-formed in the required
  constraint context, and returns a newly created constraint restricting the variable
  to be equal to the right-hand side type.

\vspace{\baselineskip}
As one can see, the unification procedure is standard, 
except that it makes sure that the resulting instantiations agree with the input
constraint context $[[Θ]]$.  As a subroutine, the unification algorithm only uses the (unification) 
constraint merge procedure and the well-formedness checking.

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\subsection{Constraint Merge}
\label{sec:constraint-merge}

In this section, we discuss the constraint merging procedure.
It allows one to combine two constraint sets into one. 
A simple union of two constraint sets is not sufficient, 
since the resulting set must not contain two entries restricting the 
same algorithmic variable---we call such entries \emph{matching}.
The matching entries must be combined into \emph{one} constraint entry, that 
would represent their conjunction. 
This way, to merge two constraint sets, we unite the entries of two
sets, and then merge the matching pairs.

\paragraph{Merging Matching Constraint Entreies}
Two \emph{matching} entries formed in the same context $[[Γ]]$ 
can be merged as shown in \cref{fig:merge-entries}.
Suppose that $[[scE1]]$ and $[[scE2]]$ are input entries. 
The result of the merge $[[scE1 & scE2]]$ must be the 
weakest entry which implies both  $[[scE1]]$ and $[[scE2]]$.

\begin{figure}[h]
  \ottdefnSCMELabeled\\
  \caption{Merge of Matching Constraint Entries}
  \label{fig:merge-entries}
\end{figure}

Suppose that one of the input entries, say $[[scE1]]$, is a unification 
constraint entry. Then the resulting entry $[[scE1]]$ must coincide with it 
(up-to-equivalence), and thus, it is only required to check that $[[scE2]]$ 
is implied by $[[scE1]]$.
\begin{itemize}
  \item If $[[scE2]]$ is also a restricting entry, then the types on the right-hand side
    of $[[scE1]]$ and $[[scE2]]$ must be equivalent,
    as given by rules \ruleref{\ottdruleSCMEPEqEqLabel} and \ruleref{\ottdruleSCMENEqEqLabel}.
  \item If $[[scE2]]$ is a supertype constraint $[[α̂⁺ :≥ iP]]$,
    the algorithm must check that the type assigned by $[[scE1]]$ is a supertype of $[[iP]]$.
    The corresponding symmetric rules are \ruleref{\ottdruleSCMESupEqLabel} and \ruleref{\ottdruleSCMEEqSupLabel}.
\end{itemize}

If both input entries are supertype constraints: $[[α̂⁺ :≥ iP]]$ and $[[α̂⁺ :≥ iQ]]$,
then their conjunction is $[[α̂⁺ :≥ iP ∨ iQ]]$, as given by \ruleref{\ottdruleSCMESupSupLabel}.
The least upper bound---$[[iP ∨ iQ]]$ is the least supertype of both $[[iP]]$ and $[[iQ]]$,
and this way, $[[α̂⁺ :≥ iP ∨ iQ]]$ is the weakest constraint entry that implies
$[[α̂⁺ :≥ iP]]$ and $[[α̂⁺ :≥ iQ]]$. The algorithm finding the least upper bound
is discussed in \cref{sec:lub}.

\paragraph{Merging Constraint Sets}
  The algorithm for merging constraint sets is shown in \cref{fig:merge-subtyping-constraints}.
  As discussed, the result of merge $[[SC1]]$ and $[[SC2]]$ consists of three parts: 
  \begin{enumerate*}
    \item[(i)] the entries of $[[SC1]]$ that do not match any entry of $[[SC2]]$;
    \item[(ii)] the entries of $[[SC2]]$ that do not match any entry of $[[SC1]]$; and
    \item[(iii)] the merge (\cref{fig:merge-entries}) of matching entries.
  \end{enumerate*}


\begin{figure}[h]
  Suppose that $[[Θ ⊢ SC1]]$ and $[[Θ ⊢ SC2]]$.\\
  Then $[[Θ ⊢ SC1 & SC2 = SC]]$
  defines a set of constraints $[[SC]]$ such that $[[scE]] \in [[SC]]$ iff either:
  \begin{itemize}
    \item $[[scE]] \in [[SC1]]$ and there is no matching $[[scE']] \in [[SC2]]$; or
    \item $[[scE]] \in [[SC2]]$ and there is no matching $[[scE']] \in [[SC1]]$; or
    \item $[[Θ(α̂±) ⊢ scE1 & scE2 = scE]]$ for some $[[scE1]] \in [[SC1]]$ and $[[scE2]] \in [[SC2]]$
      such that $[[scE1]]$ and $[[scE2]]$ both restrict variable $[[α̂±]]$. 
  \end{itemize}

  \caption{Constraint Merge}
  \label{fig:merge-subtyping-constraints}
\end{figure}

As shown in \cref{fig:merge-entries}, the merging procedure relies 
substantially on the least upper bound algorithm.
In the next section, we discuss this algorithm in detail,
together with the upgrade procedure, selecting the least supertype 
ell-formed in a given context.

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\subsection{Type Upgrade and the Least Upper Bounds}
\label{sec:lub}

Both type upgrade and the least upper bound algorithms are used
to find a minimal supertype under certain conditions. 
For a given type $[[iP]]$ well-formed in $[[Γ]]$, the \emph{upgrade} operation 
finds the least among those supertypes of $[[iP]]$ that are well-formed
in a smaller context $[[Δ ⊆ Γ]]$.
For given two types $[[iP1]]$ and $[[iP2]]$ well-formed in $[[Γ]]$,
the \emph{least upper bound} operation finds the least among
common supertypes of $[[iP1]]$ and $[[iP2]]$ well-formed in $[[Γ]]$.
These algorithms are shown in \cref{fig:type-upgrade}.

\begin{figure}[h]
  \begin{multicols}{2}
    \ottdefnLUBUpLabeled{}
    \columnbreak\\
    \ottdefnLUBNsubLabeled{}
  \end{multicols}
  \caption{Type Upgrade and Leas Upper Bound Algorithms}
  \label{fig:type-upgrade}
\end{figure}

\paragraph{The Type Upgarde}
The type upgrade algorithm uses the least upper bound algorithm as a subroutine.
It exploits the idea that the free variables of a positive type $[[iQ]]$
cannot disappear in its subtypes (see \cref{prop:subtyping-preserves-fv}). 
It means that if 
a type $[[iP]]$ has free variables not occurring 
in $[[iP']]$, then any common supertype of $[[iP]]$
and $[[iP']]$ must not contain these variables either.
This way, any supertype of $[[iP]]$
not containing certain variables $[[pnas]]$ must also be 
a supertype of $[[iP' = [pnbs/pnas]iP ]]$, where $[[pnbs]]$ are fresh;
and vice versa: any common supertype of $[[iP]]$ and $[[iP']]$
does not contain $[[pnas]]$ nor $[[pnbs]]$.

This way, to find the least supertype of $[[iP]]$ well-formed in $[[Δ]] = [[Γ \ {pnas}]]$
(\ie not containing $[[pnas]]$), we can do the following.
First, construct a new type $[[iP']]$ by renaming $[[pnas]]$ in $[[iP]]$ to fresh $[[pnbs]]$,
and second, find \emph{the least upper bound} of $[[iP]]$ and $[[iP']]$ in the appropriate
context. However, for reasons of symmetry, in rule
\ruleref{\ottdruleLUBUpgradeLabel} we employ a different but equivalent approach:
we create \emph{two} types $[[iP1]]$ and $[[iP2]]$ constructed by renaming $[[pnas]]$ in $[[iP]]$
to fresh disjoint variables $[[pnbs]]$ and $[[pncs]]$ respectively, and then 
find the least upper bound of $[[iP1]]$ and $[[iP2]]$.

\paragraph{The Least Upper Bound}
The Least Upper Bound algorithm we use operates on \emph{positive}
types. This way, the inference rules of the algorithm
analyze the three possible shapes of the input types:
a variable type, an existential type, and a shifted computation.

 Rule \ruleref{\ottdruleLUBExistsLabel} covers the case when 
 at least one of the input types is an existential type.
 In this case, we can simply move the existential quantifiers
 from both sides to the context, and make a tail-recursive call.
 However, it is important to make sure that 
 the quantified variables $[[nas]]$ and $[[nbs]]$ are disjoint
 (\ie alpha-renaming might be required in the implementation).
  
 Rule \ruleref{\ottdruleLUBVarLabel} applies when 
 both sides are variables. In this case,
 the common supertype only exists if these variables are 
 the same. And if they are, the common supertypes
 must be equivalent to this variable.

Rule \ruleref{\ottdruleLUBShiftLabel} is the most 
interesting. If both sides are not quantified, and one of the sides is 
a shift, so must be the other side. 
However, the set of common upper bounds is not trivial in this case.
For example, $[[↓(β⁺ → γ1⁻)]]$ and $[[↓(β⁺ → γ2⁻)]]$ have
two non-equivalent common supertypes: 
$[[∃α⁻.↓α⁻]]$ 
(by instantiating $[[α⁻]]$ with $[[β⁺ → γ1⁻]]$ and $[[β⁺ → γ2⁻]]$ respectively)
and 
$[[∃α⁻.↓(β⁺ → α⁻)]]$ 
(by instantiating $[[α⁻]]$ with $[[γ1⁻]]$ and $[[γ2⁻]]$ respectively).
As one can see, the second supertype $[[∃α⁻.↓(β⁺ → α⁻)]]$ is the least among them
because it abstracts over a `deeper' negative subexpression.

In general, we must 
\begin{itemize*}
  \item[(i)] find the most detailed pattern (a type with `holes' at negative positions) 
    that matches both sides, and 
  \item[(ii)] abstract over the `holes' by existential quantifiers.
\end{itemize*}
The algorithm that finds the most detailed common pattern is called \emph{anti-unification}.
As output, it returns $[[(Ξ, uP, aus1, aus2)]]$, where important for us is
$[[uP]]$---the pattern and $[[Ξ]]$---the set of `holes' represented by negative algorithmic variables.
We discuss the anti-unification algorithm in detail in the following section.


%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\subsection{Anti-Unification}
\label{sec:antiunification}

The anti-unification algorithm \cite{plotkin1970:generalization,reynolds1970:transform}, 
is a procedure dual to unification
For two given (potentially different) expressions, 
it finds the most specific generalizer---the most detailed pattern that 
matches both of the input expressions. As evidence, it can also return 
two substitutions that instantiate the `holes' of the pattern to the input expressions.

In our case, we have to be more demanding on the anti-unification algorithm.
Since we use it to construct an existential type, whose (negative) quantified variables
can only be instantiated with negative types, we must make sure that the pattern 
has `holes' only at negative positions. Moreover, we must make sure that
the resulting substitutions for the `holes' are well-formed in the initial contest, 
and do not contain variables bound later. For example, 
the anti-unification of $[[iN1 = ∀β⁺.α1⁺ → ↑β⁺]]$ and
$[[iN2 = ∀β⁺.α2⁺ → ↑β⁺]]$
must result in a `hole', which we model as an algorithmic type variable $[[γ̂⁻]]$, 
with a pair of substitutions $[[γ̂⁻ ↦ iN1]]$ and $[[γ̂⁻ ↦ iN2]]$.
But it cannot be more specific such as
$[[∀β⁺.γ̂⁺ → ↑β⁺]]$ (since the hole cannot be positive)
or $[[∀β⁺.γ̂⁻]]$ 
(since the instantiation cannot capture the bound variable $[[β⁺]]$).

The algorithm that finds the most specific generalizer of two types
under required conditions is given in \cref{fig:anti-unification}.
It consists of two mutually recursive procedures:
the positive and the negative anti-unification. 
As the positive and the negative anti-unification procedures
are symmetric in their interface, let us discuss how to read
the positive judgment. 

The positive anti-unification judgment has form
$[[Γ ⊨ iP1 ≈au iP2 ⫤ ( Ξ , uQ , aus1 , aus2 )]]$.
As an input, it takes a context $[[Γ]]$, in which the `holes'
instantiations must be well-formed, 
and two positive types: $[[iP1]]$ and $[[iP2]]$;
it returns a tuple of four components:
$[[Ξ]]$---a set of `holes' represented by negative algorithmic variables,
$[[uQ]]$---a pattern represented as a positive algorithmic type,
whose algorithmic variables are in $[[Ξ]]$,
and two substitutions $[[aus1]]$ and $[[aus2]]$
instantiating the variables from $[[Ξ]]$ such that
$[[ [aus1]uQ = iP1 ]]$ and $[[ [aus2]uQ = iP2 ]]$. 

\begin{figure}[h]
    \ottdefnAUAUPLabeled{}
    \hfill\\
    \hfill\\
    \ottdefnAUAUNLabeled{}
    \caption{Anti-Unification Algorithm}
    \label{fig:anti-unification}
\end{figure}

At the high level, the algorithm scheme follows
the standard approach \cite{plotkin1970:generalization}
represented as a recursive procedure. Specifically, it follows two principles:
\begin{enumerate}
  \item[(i)] if the input terms start with the same constructor,
    we anti-unify the corresponding parts recursively and 
    unite the results. This principle is followed by 
    all the rules except \ruleref{\ottdruleAUAULabel},
    which works as follows:
  \item[(ii)] if the first principle does not apply to the input terms $[[iN]]$
    and $[[iM]]$ (for instance, if they have different outer constructors), the
    anti-unification algorithm returns a `hole' such that one substitution maps
    it to $[[iN]]$ and the other maps it to $[[iM]]$. This `hole'
    should have a name uniquely defined by the pair $([[iN]], [[iM]])$, so
    that it automatically merges with other `holes' mapped to the same
    pair of types, and thus, the initiality of the generalizer is ensured. 
\end{enumerate}

Let us discuss the specific rules of the algorithm in detail.

\paragraph{Variables} 
  Rules \ruleref{\ottdruleAUPVarLabel} and
  \ruleref{\ottdruleAUNVarLabel} generalize two equal variables.
  In this case, the resulting pattern is the variable itself,
  and no `holes' are needed.

\paragraph{Shifts} 
  Rules \ruleref{\ottdruleAUShiftDLabel} and
  \ruleref{\ottdruleAUShiftULabel} operate by congruence:
  they anti-unify the bodies of the shifts recursively and add
  the shift constructor back to the resulting pattern.

\paragraph{Quantifiers}
  Rules \ruleref{\ottdruleAUForallLabel} and
  \ruleref{\ottdruleAUExistsLabel} are symmetric. 
  They generalize two quantified types congruently, 
  similarly to the shift rules. 
  However, we also require that the quantified variables
  are fresh, and that the left-hand side variables are 
  equal to the corresponding variables on the right-hand side.
  To ensure it, alpha-renaming might be required in the
  implementation.

  Notice that the context $[[Γ]]$ is \emph{not} extended with 
  the quantified variables. In this algorithm, $[[Γ]]$ 
  does not play the role of a current typing context, but rather
  a snapshot of a context at the moment of calling the anti-unification,
  \ie the context in which the instantiations of the `holes' 
  must be well-formed.

\paragraph{Functions}
  Rule \ruleref{\ottdruleAUArrowLabel} congruently generalizes two function types. 
  An arrow type is the only binary constructor, 
  and thus, it is the only rule where the union of the anti-unification results is substantial.
  The interesting is the case when the resulting generalization of the 
  input types and the resulting generalization of the output types 
  have `holes' mapped to the same pair of types. 
  In this case, the algorithm must merge the `holes' into one.
  For example, the anti-unification of 
  $[[↓α⁻ → α⁻]]$ and $[[↓β⁻ → β⁻]]$
  must result in $[[↓γ̂⁻ → γ̂⁻]]$,
  rather than $[[↓γ1̂⁻ → γ2̂⁻]]$.

  In our representation of the anti-unification algorithm, this `merge' happens
  automatically:
  following the rule \ruleref{\ottdruleAUAULabel},
  the name of the `hole' is uniquely defined by the pair of types it is mapped to.  
  Specifically, when anti-unifying $[[↓α⁻ → α⁻]]$ and $[[↓β⁻ → β⁻]]$ our algorithm returns 
  $[[↓α̂⁻_{α⁻, β⁻} → α̂⁻_{α⁻, β⁻}]]$, that is a renaming of $[[↓γ̂⁻ → γ̂⁻]]$.

  This way, as the output the rule returns the following tuple:
  \begin{itemize}
    \item $[[Ξ1 ∪ Ξ2]]$---a simple union of the sets of `holes' 
      returned from by the recursive calls, 
    \item $[[uQ → uM]]$---the resulting pattern 
      constructed from the patterns returned recursively.
    \item $[[aus1 ∪ aus1']]$ and $[[aus2 ∪ aus'2]]$
      --- a union (in a relational sense)
      of the substitutions returned by the recursive calls. 
      It is worth noting that the union is well-defined because
      the result of the substation on a `hole' is determined by the 
      name of the `hole'.
  \end{itemize}

\paragraph{The Anti-Unification Rule}
  Rule \ruleref{\ottdruleAUAULabel} is the base case of the anti-unification
  algorithm. If the congruent rules are not applicable, 
  it means that the input types have a substantially different structure,
  and thus, the only option is to create a `hole'. 
  There are three important aspects of this rule that we would like to discuss.

  First, as mentioned earlier, the freshly created `hole' has a name
  that is uniquely defined by the pair of input types.
  It is ensured by the following invariant:
  all the `holes' in the algorithm have name $[[α̂⁻]]$ 
  indexed by the pair of negative types it is mapped to.
  This way, the returning set of `holes' is a singleton set
  $\{ [[ â⁻_{iN, iM} ]]\}$; the resulting 
  pattern is the `hole' $[[â⁻_{iN, iM}]]$,
  and the mappings simply send it to the corresponding types:
  $[[â⁻_{iN, iM} ↦ iN]]$ and $[[â⁻_{iN, iM} ↦ iM]]$.

  Second, this rule is only applicable to negative types, 
  moreover, the input types are checked to be well-formed 
  in the outer context $[[Γ]]$. This is required by the 
  usage of anti-unification: we 
  call it to build an existential type that would be 
  an upper bound of two input types via abstracting
  some of their subexpressions under existential quantifiers.
  The existentials quantify over \emph{negative} variables,
  and they must be instantiated in the context
  available at that moment.

  Third, the rule is only applicable if all other rules fail. 
  Notice that it could happen even when the input types have matching constructors. 
  For example, the generalizer of $[[↑α⁺]]$ and
  $[[↑β⁺]]$ is $[[γ̂⁻]]$ (with mappings $[[γ̂⁻ ↦ ↑α⁺]]$ and $[[γ̂⁻ ↦ ↑β⁺]]$),
  rather than $[[↑γ̂⁺]]$. This way, the algorithm must try to 
  apply the congruent rules first, and only if they fail,
  apply \ruleref{\ottdruleAUAULabel}.
  This principle makes the inference system not syntax-directed:
  it is not known a priori (before the recursive call) 
  whether the corresponding congruent rule or rule \ruleref{\ottdruleAUAULabel} 
  will be applied.

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\subsection{Type Inference}
\label{sec:typing}

Finally, we present the type inference algorithm. 
Similarly to the subtyping algorithm, it structurally corresponds to the declarative
inference specification, meaning that most of the algorithmic rules have
declarative counterparts, with respect to which they are sound and complete.

This way, the inference algorithm also consists of three mutually recursive procedures: 
the positive type inference, the negative type inference, and the
application type inference. As subroutines, the inference algorithm uses
subtyping, constraint merge, and normalization.  
The corresponding graph is shown in \cref{fig:alg-typing-graph}.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    [>={Stealth[scale=2]},node distance=2.2cm,every node/.style={draw,rectangle},every text node part/.style={align=center}]
    % Define nodes
    \node[] (1) {Positive Inference\\$[[Γ; Φ ⊨ v : iP]]$};
    \node[right=1.2cm of 1] (2) {Negative Inference\\$[[Γ; Φ ⊨ c : iN]]$};
    \node[right=1.2cm of 2] (3) {Application Inference\\$[[Γ ; Φ ; Θ1 ⊨ uN ● args ⇒> uM ⫤ Θ2 ; SC]]$};
    \node[below=0.6cm of 2] (7) {Minimal Instantiation\\$[[Γ ⊢ uP SC minby uσ]]$\\(\cref{sec:constraint-singularity})};
    \node[below=0.5cm of 7] (8) {Constraint Singularity\\$[[SC singular with uσ]]$\\(\cref{sec:constraint-singularity})};
    \node[right=0.3cm of 7] (4) {Constraint Merge\\$[[Θ ⊢ SC1 & SC2 = SC3]]$\\(\cref{sec:constraint-merge})};
    \node[left=0.3cm of 7] (6) {Negative Subtyping\\$[[Γ ; Θ ⊨ uN1 ≤ iN2 ⫤ SC]]$\\(\cref{sec:subtyping-algorithm})};
    \node[below=0.9cm of 6] (5) {Positive Subtyping\\$[[Γ ; Θ ⊨ uP1 ≥ iP2 ⫤ SC]]$\\(\cref{sec:subtyping-algorithm})};
    \draw[->] ([yshift=-.2cm]2.west) to ([yshift=-.2cm]1.east);
    \draw[->] ([yshift=+.2cm]1.east) to ([yshift=+.2cm]2.west);
    \draw[->, bend right=20] (3) to (1);
    \draw[->] (2) to (3);
    \draw[->] (2) to (4);
    \draw[->] (2) to (7);
    \draw[->] (3) to (4);
    \draw[->] (7) to (8);
    \draw[->] (1.south west) |- (5);
    \draw[->] (3) |- (5.south east);
    \draw[->] (2) to (6);
  \end{tikzpicture}  
  \caption{Dependency graph of the typing algorithm}
  \label{fig:alg-typing-graph}
\end{figure}

The positive and the negative type inference judgments have symmetric forms:
$[[Γ; Φ ⊨ v : iP]]$ and $[[Γ; Φ ⊨ c : iN]]$. Both of these algorithms
take as an input typing context $[[Γ]]$, a variable context $[[Φ]]$, and 
a term (a value or a computation) taking its type variables from $[[Γ]]$, 
and term variables from $[[Φ]]$. As an output, they return a type
of the given term, which we guarantee to be normalized. 

The application type inference judgment has form 
$[[Γ ; Φ ; Θ1 ⊨ uN ● args ⇒> uM ⫤ Θ2 ; SC]]$.
As an input, it takes three contexts: typing context $[[Γ]]$, a variable context $[[Φ]]$,
and a constraint context $[[Θ1]]$. It also takes a head type $[[uN]]$ and 
a list of arguments (terms) $[[args]]$ the head is applied to.
The head may contain algorithmic variables specified by $[[Θ1]]$, 
in other words, $[[Γ; dom(Θ1) ⊢ uN]]$.
As a result, the application inference judgment returns 
$[[uM]]$---a normalized type of the result of the application.
Type $[[uM]]$ may contain new algorithmic variables, and thus, 
the judgment also returns $[[Θ2]]$---an updated constraint context
and $[[SC]]$---a set of subtyping constraints.
Together $[[Θ2]]$  and $[[SC]]$ specify how the algorithmic variables 
must be instantiated.


\begin{figure}
  \ottdefnATPInfLabeled{}
  \hfill \\
  \ottdefnATNInfLabeled{}
  \hfill \\
  \ottdefnATSpinInfLabeled{}
  \caption{Algorithmic Type Inference}
  \label{fig:type-inference}
\end{figure}


The inference rules are shown in
\cref{fig:type-inference}.
Next, we discuss them in detail.

\paragraph{Variables}
  Rule \ruleref{\ottdruleATVarLabel} 
  infers the type of a positive variable by looking it up in the 
  term variable context and normalizing the result.

\paragraph{Annotations}
  Rules \ruleref{\ottdruleATPAnnotLabel} and \ruleref{\ottdruleATNAnnotLabel}
  are symmetric.
  First, they check that the annotated type is well-formed in the 
  given context $[[Γ]]$. Then they make a recursive call to infer the 
  type of annotated expression, check that the inferred type is a subtype of 
  the annotation, and return the normalized annotation.

\paragraph{Abstractions}
  Rule \ruleref{\ottdruleATtLamLabel} infers the type of a lambda abstraction.
  It checks the well-formedness of the annotation $[[iP]]$,
  makes a recursive call to infer the type of the body in the extended context, 
  and returns the corresponding arrow type.
  Since the annotation $[[iP]]$ is allowed to be non-normalized,
  the rule also normalizes the resulting type.

  Rule \ruleref{\ottdruleATTLamLabel} infers the type of a big lambda.
  Similarly to the previous case, it makes a recursive call to infer the type
  of the body in the extended \emph{type} context. 
  After that, it returns the corresponding universal type. 
  It is also required to normalize the result.  
  For instance, if $[[α⁺]]$ does not occur in the body of the lambda,
  the corresponding $[[∀]]$ will be removed.

\paragraph{Return and Thunk}
  Rules \ruleref{\ottdruleATThunkLabel} and \ruleref{\ottdruleATReturnLabel}
  are similar to the declarative rules: they make a recursive call
  to type the body of the thunk or the return expression and
  put the shift on top of the result.

\paragraph{Unpack}
  Rule \ruleref{\ottdruleATUnpackLabel}
  allows one to unpack an existential type.
  First, it infers the existential type $[[∃nas.iP]]$ of the value being unpacked,
  and since the type is guaranteed to be normalized, binds 
  the quantified variables with $[[nas]]$.
  Then it infers the type of the body in the appropriately extended context
  and checks that the inferred type does not depend on $[[nas]]$
  by checking well-formedness $[[Γ ⊢ iN]]$.

\paragraph{Let Binders}
  Rule \ruleref{\ottdruleATVarLetLabel}
  represents the type inference of a standard let binder.
  It infers the type of the bound value $[[v]]$, and
  makes a recursive call to infer the type of the body in the extended context.

  Rule \ruleref{\ottdruleATAppLetAnnLabel}
  infers a type of \emph{annotated} applicative let binder.
  First, it infers the type of the head of the application,
  ensuring that it is a \emph{thunked computation} $[[↓iM]]$.
  After that, it makes a recursive call
  to the application inference procedure,
  returning an algorithmic type $[[iM']]$, 
  that must be a subtype of the annotation $[[↑uP]]$.

  Then premise $[[Γ; Θ ⊨ iM' ≤ ↑iP ⫤ SC2]]$
  together with $[[Θ ⊢ SC1 & SC2 = SC]]$
  check that $[[iM']]$ can be instantiated to the annotated type $[[↑iP]]$,
  and if it is, the algorithm infers the type of the body in the extended context,
  and returns it as the result. 

  Rule \ruleref{\ottdruleATAppLetLabel}
  works similarly to \ruleref{\ottdruleATAppLetAnnLabel}.
  However, since no annotation is given,
  the algorithm must ensure that the inferred $[[uQ]]$
  has the `canonical' minimal instantiation.
  To find it, it makes a call to the minimal instantiation algorithm 
  (\cref{sec:minimal-instantiation})
  that finds the substitution that satisfies the inferred constraints $[[SC]]$ and
  instantiates $[[uQ]]$ to the minimal (among other such instantiations)
  type $[[ [uσ]uQ ]]$.

\paragraph{Application to an Empty List of Arguments}
  Rule \ruleref{\ottdruleATEmptyAppLabel}
  is the base case of application inference. 
  If the list of applied arguments is empty, 
  the inferred type is the type of the head,
  and the algorithm returns it after normalizing.

\paragraph{Application of a Polymorphic Type $[[∀]]$}
  Rule \ruleref{\ottdruleATForallAppLabel},
  analogously to the declarative case,
  is the rule ensuring the implicit elimination of the universal quantifiers. 
  This is the place where the algorithmic variables are introduced.
  The algorithm simply replaces the quantified variables 
  $[[pas]]$ with fresh algorithmic variables $[[puas]]$,
  and makes a recursive call in the extended context.

  To ensure that this step does not cause infinite recursion, 
  we also check that the head type has at least one 
  $[[∀]]$-quantifier. Also, to force the algorithm to
  apply rule \ruleref{\ottdruleATEmptyAppLabel} when  
  there are no arguments, we require $[[args ≠ ·]]$.

\paragraph{Application of an Arrow Type}
  Rule \ruleref{\ottdruleATArrowAppLabel}
  is the main rule of algorithmic application inference.
  It is applied when the head has an arrow type $[[uQ → uN]]$.
  First, it infers the type of the first argument $[[v]]$,
  and then, calling the algorithmic subtyping,
  finds $[[SC1]]$---the minimal constraint ensuring that 
  $[[uQ]]$ is a supertype of the type of $[[v]]$.
  Then it makes a recursive call applying $[[uN]]$ to the rest of the arguments 
  and merges the resulting constraint with $[[SC1]]$.

\subsection{Minimal Instantiation and Constraint Singularity}
\label{sec:constraint-singularity}

Multiple types $[[iM]]$ can be inferred for a type application:
$[[Γ ; Φ ⊢ iN ● args ⇒> iM]]$ but only one \emph{principal} 
type should be chosen for a variable in an unannotated let binder.
Declaratively, we require the principal type $[[iP]]$ to be
minimal among other all types $[[iP']]$ that can be inferred
for the application $[[Γ ; Φ ⊢ iN ● args ⇒> ↑iP']]$.
Algorithmically, the inference returns an
\emph{algorithmic} type $[[uP]]$ 
together with a set of necessary and sufficient constraints $[[SC]]$
that any instantiation of $[[uP]]$ must satisfy.
This way, to guarantee principality, we must minimize the instantiation of $[[uP]]$,
while ensuring that it satisfies the given constraints $[[SC]]$. 

The minimal instantiation judgment `$[[Γ ⊢ uP SC minby uσ]]$'
means that $[[uσ]]$ is the minimum among all the substitutions satisfying $[[SC]]$,
that is $[[uσ]]$ instantiates $[[uP]]$ to a subtype of any other 
instantiation of $[[uP]]$. The algorithm is given in \cref{fig:minimal-instantiation}.
First, it removes the existential quantifiers by \ruleref{\ottdruleSINGExistsLabel}.
Then it considers two cases. If the type is an algorithmic variable $[[α̂⁺]]$
restricted by a constraint $[[(α̂⁺ :≥ iP) ∊ SC]]$,
its minimal instantiation is the type $[[nf(iP)]]$, 
and \ruleref{\ottdruleSINGPUvarLabel} is applied.
Otherwise, \ruleref{\ottdruleSINGSingLabel} is applied,
meaning that all the instantiations of the type must be equivalent.
The algorithm ensures that by two premises:
\begin{enumerate*}
  \item[(i)] checking that all the algorithmic variables of the type are restricted by the constraint set; and
  \item[(ii)] building the substitution that satisfies the constraint set on these variables 
    (and simultaneously checking that such substitution is unique up-to-equivalence)
    using the constraint singularity algorithm.
\end{enumerate*}

\begin{figure}[h]
  \hfill\\
  \ottdefnMININSTLabeled
  \caption{Minimal Instantiation}
  \label{fig:minimal-instantiation}
\end{figure}


The singularity algorithm 
performs two tasks: it checks that a
\emph{constraint set} has a single substitution satisfying it, 
and if it does, it builds this substitution.

To implement the singularity algorithm, we define a partial function $[[SC
singular with uσ]]$, taking a subtyping constraint $[[SC]]$ as an argument and
returning a substitution $[[uσ]]$---the only solution of $[[SC]]$. The
constraint $[[SC]]$ consists of constraint entries. Thus, we define the
singularity using the singularity of constraint entries: each entry of $[[SC]]$
must have a unique instantiation and the resulting substitution $[[uσ]]$ must be
the combination of these instantiations (see \cref{fig:constraint-singularity}).

\begin{figure}[h]
  For a constraint set $[[SC]]$, we write $[[SC singular with uσ]]$ if and only if:
  \begin{itemize}
    \item for any \emph{positive} constraint entry $[[scE ∊ SC]]$, there exists $[[iP]]$ such that $[[scE singular with iP]]$,
      and for the variable $[[β̂⁺]]$ restricted by $[[scE]]$, $[[ [uσ]β̂⁺ = iP ]]$;
    \item the symmetric property holds for all \emph{negative} $[[scE ∊ SC]]$; and
    \item for any $[[α̂± ∉ dom(SC)]]$, $[[ [uσ]α̂± = α̂± ]]$.
  \end{itemize}
  \caption{Constraint Singularity}
  \label{fig:constraint-singularity}
\end{figure}

The singularity of constraint \emph{entries} is defined in
\cref{fig:constraint-entry-singularity}. The \emph{equivalence} entries are
always singular, as the only possible type satisfying them is the one given in
the constraint itself, which is reflected in \ruleref{\ottdruleSINGNEqLabel} and
\ruleref{\ottdruleSINGPEqLabel}. The \emph{subtyping} constraints are trickier.
As will be discussed in \cref{sec:proof-lub-upgrade}, variables 
(and equivalent types such as $[[∃nas.pa]]$) do not have 
\emph{proper} supertypes, and thus, the constraints of a form 
$[[pua :≥ ∃nas.pa]]$ are singular with the only possible normalized solution $[[pa]]$
\ruleref{\ottdruleSINGSupVarLabel}. However, if the body of the existential type
is guarded by a shift $[[∃nas.↓iN]]$, it is singular if and only if $[[iN]]$ is
equivalent to some $[[αi⁻ ∊ {nas}]]$ bound by the quantifier, see the rule
\ruleref{\ottdruleSINGSupShiftLabel}.

\begin{figure}[h]
  \hfill\\
  \ottdefnSINGLabeled
  \caption{Singular Constraint Entry}
  \label{fig:constraint-entry-singularity}
\end{figure}
