Over the last half-century, there has been considerable work on developing type
inference algorithms for programming languages,  mostly aimed at solving the
problem of \emph{type polymorphism}.

That is, in pure polymorphic lambda
calculus---\systemf\cite{girard-system-f,jcr-system-f}---the polymorphic type
$[[∀a.fA]]$ has a big lambda $[[Λa.fe]]$ as an introduction form, and an
explicit type application $[[ fe[fA] ]]$ as an elimination form. This is an
extremely simple and compact type system, whose rules fit on a single page, but
whose semantics are advanced enough to accurately represent concepts such as
parametricity and representation independence. However, \systemf by itself is
unwieldy as a programming language. The fact that the universal type $[[∀a.fA]]$
has explicit eliminations means that programs written using polymorphic types
will need to be stuffed to the gills with type annotations explaining how and
when to instantiate the quantifiers.

Therefore, most work on type inference has been aimed at handling type
instantiations implicitly---we want to be able to use a polymorphic function
like $[[len : ∀a.List a → Int]]$ at any concrete list type without explicitly
instantiating the quantifier in $[[len]]$'s type. That is, we want to write
$[[ len [one,two,three] ]]$ instead of writing
$[[ len [Int] [one,two,three] ]]$.

The way this is typically done is by using a \emph{subtyping} relation, induced
by the polymorphic instantiation: the type $[[∀a.fA]]$ is a \emph{subtype} of
all of its instantiations. So we wish to be free to use the same polymorphic
function $[[len]]$ at many different types such as $[[List Int → Int]]$, $[[List
Bool → Int]]$, $[[List Int × Bool → Int]]$, and so on. However, the subtyping
can be used nondeterministically: whenever we see a polymorphic type
$[[∀a.fA]]$, we know it is a subtype of \emph{any} of its instantiations. To
turn this into an algorithm, we have to actually make some choices. 

The most famous of the algorithms for handling this is the Damas-Hindley-Milner
(DHM) algorithm \cite{hindley69:principal,milner78:theory,damas82:principal}.
DHM deals with this choice using \emph{unification}. Whenever we would have had
to guess a particular concrete type in the specification, the DHM algorithm
introduces a \emph{unification variable}, and incrementally instantiates this variable
as more and more type constraints are observed.

However, the universal quantifier is not the only quantifier! Dual to the
universal quantifier $\forall$ is the existential quantifier $\exists$. Even
though existential types have an equal logical status to universal types, they
have been studied much less frequently in the literature. The most widely-used
algorithm for handling existentials is the algorithm of Odersky and L{\"a}ufer
\cite{laufer94:polymorphic}. This algorithm treats existentials in a
second-class fashion: they are not explicit connectives of the logic but rather
are tied to datatype declarations. As a result, packing and unpacking
existential types is tied to constructor introductions and constructor
eliminations in pattern matches. This allows Damas-Milner inference to continue
to work almost unchanged, but it does come at the cost of losing first-class
existentials and also of restricting the witness types to monomorphic types
(\ie types with no quantifiers inside of them).

There has been a limited amount of work on support for existential types as a
first-class concept. In an unpublished draft, \citet{leijen06:first-class}
studied an extension of Damas-Milner inference in which type schemes contain
alternating universal and existential quantifiers. Quantifiers still range over
monotypes, so higher-rank 
polymorphism is not permitted: one cannot instantiate a quantifier with a type
that itself contains quantifiers.
More recently,
\citet{dunfield16:existential} studied type inference for existential types in
the context of GADT type inference, which, while still predicative, supported
higher-rank types (i.e., quantifiers can occur anywhere inside of a type
scheme). \citet{eisenberg21:existential} propose a system which only permits
types of the form forall-exists, but which permits projective elimination in the
style of ML modules. 

All of these papers are restricted to \emph{predicative} quantification, where
quantifiers can only be instantiated with monotypes. However, existential types
in full \systemf are \emph{impredicative}---that is, quantifiers can be
instantiated with arbitrary types, specifically including types containing
quantifiers.

Historically, inference for impredicative quantification has been neglected, due
to results by \citet{tiuryn-urzczyn-96} and \citet{chrzaszcz-98} which show that
not only is full type inference for \systemf undecidable, but even that the
subtyping relation induced by type instantiation is undecidable. However, in
recent years, interest in impredicative inference has revived (for example, the
work of \citet{serrano-2020}), with a focus on avoiding the undecidability
barriers by doing \emph{partial} type inference. That is, we no longer try to
infer the type of any term in the language, but rather accept that the
programmer will need to write annotations in cases where inference would be too
difficult. Unfortunately, it is often difficult to give a specification for when
the partial algorithm succeeds---for example, \citet{eisenberg21:existential}
observe that their algorithm lacks a declarative specification, and explain why
supporting existential types makes giving a specification particularly
difficult. 

One especially well-behaved form of partial type inference is 
\emph{local type inference}, introduced by \citet{pierce2000:local}. In this approach,
quantifiers in a polymorphic function are instantiated using only the type
information available in the arguments at each call site. While this infers
fewer quantifiers than global algorithms such as Damas-Milner can, it has the
twin benefits that it is easy to give a mathematical specification to, and that
failures of type inference are easily explained in terms of local features of
the call site. In fact, many production programming languages such as C\# and
Java use a form of local type inference, due to the ease of extending this style
of inference to support object-oriented features. 

In this paper, we extend the local type inference algorithm to a language with
both universal and existential quantifiers, which can both be instantiated
impredicatively. This extended system is referred to as \fexists. 
The version of this system without existential quantifiers was studied 
by \citet{mercer2019:system-f}. However, the addition of existentials in \fexists
broke a number of the invariants that traditional type inference algorithms depend
 on and required us to invent new algorithms which combine both unification and 
 (surprisingly) anti-unification.

We make the following \emph{\textbf{contributions}}:

\paragraph*{Declarative type system specification (\cref{sec:overview,sec:declarative-system})} 
    We present \fexists---a second-order polymorphic type system that supports first-class
    existential and universal quantifiers, both of which can be instantiated
    impredicatively. To evade the undecidability results surrounding subtyping
    for \systemf, we base the system on \CBPV \cite{levy2006:cbpv}: we
    use \emph{polarization} of types and terms to formulate the restrictions
    that make typing decidable. 

    We give a \emph{declarative} specification of local type inference in this
    system (\ie the typing information is only available at a limited scope)
    based on a \emph{subtyping} relation. Our specification makes it easy to see
    what is and what is not inferrable in the system. 

    To show that our restrictions do not unduly limit the expressiveness of the
    language, we prove that every \fexists term can be embedded into \systemf
    and vice versa.

\paragraph*{Type inference algorithm (\cref{sec:algorithm})} 
  We give a type inference algorithm implementing the declarative specification.
  Because our system supports both existential and universal quantifiers, our
  implementation of the subtyping relation requires the use of
  \emph{anti-unification}---a dual to unification that finds the most specific
  generalization rather than the most general unifier. To our knowledge,
  anti-unification has never been used in the context of polymorphic type
  inference.

  The type inference algorithm itself is a purely local one, in the style of
  \citet{pierce2000:local}. The locality can be seen from the fact that 
  none of the unification variables introduced as part of type inference
  escape the scope of a single function application.

\paragraph*{Correctness of the algorithm (\cref{sec:proofs})}
  We prove that our algorithm is terminating and both sound and complete with
  respect to the declarative specification. This proof of soundness and completeness
  of the algorithm required us to introduce an intermediate system in-between
  the declarative and the algorithmic systems, and furthermore, the
  soundness and completeness proofs are mutually recursive with each other.
  The central theorems and the key proof ideas are formulated in \cref{sec:proofs}, 
  while the full definitions and formal proofs are deferred to the appendix (\cref{appendix,app:proofs}).

% \paragraph*{Extrapolation of the inference framework to other type systems (\cref{sec:extensions})}
%     We explore the design space further to show how the
%     same type inference scheme can be applied to
%     work with different type systems. In particular,
%     \begin{enumerate*}
%       \item[(i)] in combination with \emph{bidirectional} typechecking---the 
%         approach used in the original local type inference paper of 
%         \citet{pierce2000:local} that minimizes the number of needed annotations;
%       \item[(ii)] in systems with different subtyping rules, such as  
%         the one used in \citet{zhao22:elementary}---a system that
%         permits explicit type applications;
%       \item[(iii)] in systems with \emph{bounded} polymorphic quantifiers.
%         That is, one can only instantiate a quantifier with a 
%         type that is a subtype of a given bound.
%     \end{enumerate*}
  

% \ilyam{Is the following needed since we have the contributions? I just wanted to introduce the appendix somewhere...}
% \paragraph*{\textbf{Structure of this Paper}}
% In \cref{sec:overview}, we discuss the language of the \fexists system, providing
% examples, outlining limitations, and summarizing the main ideas behind the
% algorithm. \Cref{sec:declarative-system} presents a formal definition of the
% declarative specification and its key properties. In \cref{sec:algorithm}, the
% type inference algorithm is introduced, detailing a hierarchy of subroutines;
% upon perusing this section, one could \emph{implement} the type inference
% algorithm. \Cref{sec:proofs} overviews the algorithm's correctness proof,
% explaining its structure and how the declarative system's features make it
% decidable. \Cref{sec:extensions} covers potential \fexists extensions and future
% work directions. Finally, \cref{appendix} contains comprehensive formal
% definitions and proofs of lemmas and theorems used in the paper; in instances
% where the full definitions might not be of immediate interest, we direct the
% reader to the appendix.

