\label{sec:algorithm}

In this section, we present the algorithmization of the declarative system described above.
The algorithmic system follows the structure of the declarative specification closely.
First, it is also given by a set of inference rules, which, however,
are mode-correct (\cite{dunfield2020:bidirectional}), \ie
the output of each rule is always uniquely defined by its input.
And second, most of the declarative rules 
(except for the rules \ruleref{\ottdruleDTPEquivLabel} and \ruleref{\ottdruleDTNEquivLabel})
have a unique algorithmic counterpart, 
which simplifies reasoning about the algorithm and its correctness proofs.


\subsection{Algorithmic Syntax}
\label{sec:algo-syntax}

First, let us discuss the syntax of the algorithmic system. 

\begin{figure}[t]

  \begin{minipage}[t]{0.49\textwidth}
      Negative Algorithmic Variables\\
      $[[α̂⁻]]$, $[[β̂⁻]]$, $[[γ̂⁻]]$, \dots\\
  \end{minipage}%
  \begin{minipage}[t]{0.49\textwidth}
      Positive Algorithmic Variables\\
      $[[α̂⁺]]$, $[[β̂⁺]]$, $[[γ̂⁺]]$, \dots\\
  \end{minipage}

  \hfill\\
  \begin{minipage}[t]{0.49\textwidth}
      \ottuNShort
  \end{minipage}
  \begin{minipage}[t]{0.49\textwidth}
      \ottuPShort
  \end{minipage}
  \hfill\\
  Algorithmic Type Context\\
   $[[Ξ]] \Coloneqq \{[[α1̂±]], \dots, [[αn̂±]]\}$ where $[[α1̂±]], \dots, [[αn̂±]]$ are pairwise distinct \\
  \hfill\\
  Instantiation Context\\
   $[[Θ]] \Coloneqq \{[[ α1̂±[Γ1] ]], \dots, [[ αn̂±[Γn] ]]\}$ where $[[α1̂±]], \dots, [[αn̂±]]$ are pairwise distinct \\
  \caption{Algorithmic Syntax}
  \label{fig:algo-syntax}
\end{figure}

\paragraph*{Algorithmic Variables}
To design a mode-correct inference system, we slightly modify the language we operate on.
The entities (terms, types, contexts) that the algorithm manipulates we call \emph{algorithmic}. 
They extend the previously defined declarative terms and types by adding 
\emph{algorithmic type variables} (\aka unification variables). 
The algorithmic variables represent unknown types, 
which cannot be inferred immediately but are promised to be instantiated
as the algorithm proceeds.

We denote algorithmic variables as $[[α̂⁺]]$, $[[β̂⁻]]$, \dots to distinguish
them from normal variables $[[α⁺]]$, $[[β⁻]]$. In a few places, we replace the
quantified variables $[[pas]]$ with their algorithmic counterpart $[[puas]]$.
The procedure of replacing declarative variables with algorithmic ones we call
\emph{algorithmization} and denote as $[[ nuas/nas ]]$ and $[[ puas/pas ]]$. The
converse operation---\emph{dealgorithmization}---is denoted as $[[ nas/nuas ]]$ and 
is used in the least upper bound procedure (\cref{sec:lub}).

\paragraph*{Algorithmic Types}
The syntax of algorithmic types extends the declarative syntax by adding
algorithmic variables as new terminals. We add positive algorithmic variables $[[α̂⁺]]$ 
to the syntax of positive types, and negative algorithmic variables $[[α̂⁻]]$ to the 
syntax of negative types. All the constructors of the system can be applied 
to \emph{algorithmic} types, however, algorithmic variables cannot be abstracted by the
quantifiers $[[∀]]$ and $[[∃]]$.

\paragraph*{Algorithmic Contexts $[[Ξ]]$ and Well-formedness}
To specify when algorithmic types are well-formed, we define algorithmic
contexts $[[Ξ]]$ as sets of algorithmic variables. Then
$[[Γ ; Ξ ⊢ uP]]$ and $[[Γ ; Ξ ⊢ uN]]$ represent the well-formedness judgment of
algorithmic terms defined as expected. Informally, they check that all free
declarative variables are in $[[Γ]]$, and all free algorithmic variables are in
$[[Ξ]]$. Most of the rules are inherited from the well-formedness of
\emph{declarative} types: the declarative variables are checked to belong to the
context $[[Γ]]$, the quantifiers extend the context, type constructors are
well-formed congruently. As we extend the syntax with algorithmic variables, we
also add two base-case rules for them: \ruleref{\ottdruleWFATPUVarLabel} and
\ruleref{\ottdruleWFATNUVarLabel} (see \cref{fig:algo-wf}).

\begin{figure}
\begin{multicols}{2}
  $\vcenter{\hbox{\vdots}}$\\
  \ottusedrule{\ottdruleWFATPUVarLabeled{}}
  \columnbreak\\
  $\vcenter{\hbox{\vdots}}$\\
  \ottusedrule{\ottdruleWFATNUVarLabeled{}}
\end{multicols}
\caption{Well-formedness of Algorithmic Types}
\label{fig:algo-wf}
\end{figure}

\paragraph*{Instantiation Context $[[Θ]]$}
When one instantiates an algorithmic variable, one may only use type variables
\emph{available in its scope}. As such, each algorithmic variable must remember
the context at the moment when it was introduced. In our algorithm, this
information is represented by an \emph{instantiation context} $[[Θ]]$---a set of
pairs associating algorithmic variables and declarative contexts.

\paragraph*{Algorithmic Substitution}
We define the algorithmic substitution $[[uσ]]$ as a mapping from algorithmic
variables to \emph{declarative} types. The signature $[[Θ ⊢ uσ : Ξ]]$
specifies the domain and the range of $[[uσ]]$: for each variable $[[α̂±]]$
in $[[Ξ]]$, there exists an corresponding entry in $[[Θ]]$ associating 
$[[α̂±]]$ with a declarative context $[[Γ]]$ such that $[[ [uσ]α̂± ]]$
is well-formed in $[[Γ]]$. In addition, we assume that $[[uσ]]$
acts as the identity on the variables not in $[[Ξ]]$.


\paragraph*{Algorithmic Normalization}
Similarly to well-formedness, the normalization of algorithmic types is defined
by extending the declarative definition (\cref{fig:type-nf}) with the
algorithmic variables. To the rules repeating the declarative normalization, we
add rules saying that normalization is trivial on algorithmic variables.

\begin{figure}
\begin{multicols}{2}
  $\vcenter{\hbox{\vdots}}$\\
  \ottusedrule{\ottdruleNrmPUVar{}}
  \columnbreak\\
  $\vcenter{\hbox{\vdots}}$\\
  \ottusedrule{\ottdruleNrmNUVar{}}
\end{multicols}
\caption{Normalization of Algorithmic Types}
\label{fig:algo-nf}
\end{figure}

\subsection{Type Constraints}
Throughout the algorithm's operation, it gathers information about the
algorithmic type variables, represented as \emph{constraints}. These constraints
in our system can be either \emph{subtyping constraints} or \emph{unification
constraints}. As preserved by the algorithm, each subtyping constraint has a
positive shape $[[α̂⁺ :≥ iP]]$, meaning it confines a positive algorithmic
variable to be the supertype of a positive declarative type. Unification
constraints can take a positive $[[α̂⁺ :≈ iP]]$ or negative $[[α̂⁻ :≈ iN]]$
form, but algorithmic type variables cannot occur on their right-hand side. The
constraints \emph{set} is denoted as $[[SC]]$, and we presume that each
algorithmic variable can be restricted by at most one constraint.

We define $[[UC]]$ separately as a set solely containing \emph{unification}
constraints for simpler algorithm representation. The unification algorithm,
which we use as a subroutine of the subtyping algorithm, can only produce
unification constraints. A resolution of unification constraints is simpler than
that of a general constraint set. This way, this separation allows us to better
decompose the algorithm's structure, thus simplifying the inductive proofs.

  \begin{figure}[h]
    \begin{multicols}{2}
      \ottgrammartabular { 
      \ottscE 
      \ottinterrule
      \ottruleheadOneLine
        {[[SC]]}{\Coloneqq} {\ottcom{Constraint Set}}
        {\{[[scE1]], \dots, [[scEn]]\}}\ottprodnewline
      }

    \columnbreak

      \ottgrammartabular { 
      \ottucE 
      \ottprodnewline
      \ottinterrule
      \ottruleheadOneLine
        {[[UC]]}{\Coloneqq} {\ottcom{Unification Constraint Set}}
        {\{[[ucE1]], \dots, [[ucEn]]\}}\ottprodnewline
      }
    \end{multicols}

    \label{fig:syntax-e-sc}
    \caption{Constraint Entries and Sets}
  \end{figure}


\paragraph*{Auxiliary Functions}
\begin{itemize}
  \item
    We define $[[dom(SC)]]$---the domain of constraint set $[[SC]]$---as a set of
    algorithmic variables that it restricts. Similarly, we define $[[dom(Θ)]]$---the
    domain of instantiation context---as a set of algorithmic variables that
    $[[Θ]]$ associates with their contexts.  
  \item We write $[[Θ(α̂±)]]$ to denote the
    declarative context associated with $[[α̂±]]$ in $[[Θ]]$. 
  \item $[[uv(uN)]]$ and $[[uv(uP)]]$ denote the set of free algorithmic variables in $[[uN]]$ and
    $[[uP]]$ respectively.
  \item We write $[[Γ ⊢ Θ]]$ to denote that each
    declarative context associated with an algorithmic variable in $[[Θ]]$ is 
    a subcontext (subset) of $[[Γ]]$.
\end{itemize}

\paragraph*{Equivalent Substitutions}
In the proofs of the algorithm correctness, 
we often state or require that two substitutions are equivalent
on a given set of algorithmic variables.
We denote it as $[[Θ ⊢ uσ' ≈ uσ : Ξ]]$ meaning that
for each $[[α̂±]]$ in $[[Ξ]]$, substitutions $[[uσ]]$ and $[[uσ']]$
map $[[α̂±]]$ to types equivalent in the corresponding context:
$[[ Θ(α̂±) ⊢ [uσ']α̂± ≈ [uσ]α̂± ]]$.



\paragraph*{Constraint well-formedness and satisfaction}
Suppose that $[[Θ]]$ is an instantiation context. 
We say that constraint set $[[SC]]$ is well-formed in $[[Θ]]$ 
(denoted as $[[Θ ⊢ SC]]$) 
if for every every constrain entry $[[scE ∊ SC]]$
associating a variable $[[α̂±]]$ with a type $[[iP]]$,
this type is well-formed in the corresponding declarative context $[[Θ(α̂±)]]$.

Substitution $[[uσ]]$ satisfies a constraint \emph{entry} $[[scE]]$ restricting
algorithmic variable $[[α̂±]]$ if
$[[ [σ]α̂± ]]$ can be validly substituted for $[[α̂±]]$ in $[[scE]]$ 
(so that the corresponding equivalence or subtyping holds).

Substitution $[[uσ]]$ satisfies a constraint \emph{set} $[[SC]]$
if $[[Θ ⊢ SC]]$ and \emph{each entry} of $[[SC]]$ is satisfied by $[[uσ]]$.

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\subsection{Subtyping Algorithm}
\label{sec:subtyping-algorithm}
  
  For convenience and scalability, 
  we decompose the subtyping algorithm 
  into several procedures. \Cref{fig:alg-subtyping-graph}
  shows these procedures and the dependencies between them:
  arrows denote the invocation of one procedure from another.

  Some of the procedures (in particular, the unification and the
  anti-unification) assume that the input types are normalized. Therefore, we
  call the normalization procedure before invoking them, indicating this by the
  `$\ottkw{nf}$' annotation on the arrows in \cref{fig:alg-subtyping-graph}.
  Alternatively, one could normalize the input types in the very beginning
  and preserve the normalization throughout the algorithm. However, we
  delay the normalization to the places where it is required to show that
  normalization is needed only at these stages to maintain consistent
  invariants.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    [>={Stealth[scale=2]},node distance=2.4cm,every node/.style={draw,rectangle},every text node part/.style={align=center}]


    % Define nodes
    \node[] (1) {Negative Subtyping\\$[[Γ ; Θ ⊨ uN ≤ iM ⫤ SC]]$\\(\cref{fig:alg-subtyping})};
    \node[below of=1] (3) {Positive Subtyping\\$[[Γ ; Θ ⊨ uP ≥ iQ ⫤ SC]]$\\(\cref{fig:alg-subtyping})};
    \node[left=1.5cm of 3] (2) {Constraint Merge\\$[[Θ ⊢ SC1 & SC2 = SC3]]$\\(\cref{sec:constraint-merge})};
    \node[right=1.5cm of 3] (5) {Unification\\ $[[Γ ; Θ ⊨ uN ≈u iM ⫤ UC]]$\\ $[[Γ ; Θ ⊨ uP ≈u iQ ⫤ UC]]$\\(\cref{sec:unification})};
    \node[below of=3] (4) {Upgrade\\$[[upgrade Γ ⊢ iP to Δ = iQ]]$\\(\cref{sec:lub})};
    \node[below of=2] (6) {Least Upper Bound\\$[[Γ ⊨ iP1 ∨ iP2 = iQ]]$\\(\cref{sec:lub})};
    \node[below of=6] (7) {Anti-Unification\\$[[Γ ⊨ iP1 ≈au iP2 ⫤ ( Ξ , uQ , aus1 , aus2 )]]$\\$[[Γ ⊨ iN1 ≈au iN2 ⫤ ( Ξ , uM , aus1 , aus2 )]]$\\(\cref{sec:antiunification})};
    \node[below of=5] (8) {Unification Constraint Merge\\$[[Θ ⊢ UC1 & UC2 = UC3]]$\\(\cref{sec:constraint-merge})};

    % Define edges
    \draw[->] (1) to (2);
    \draw[->] (1) to (3);
    \draw[->] (1) to node[above, draw=none]{$\ottkw{nf}$} (5);
    \draw[->] (2) to (3);
    \draw[->] (2) to (6);
    \draw[->] (3) to (4);
    \draw[->] (3) to node[above, draw=none]{$\ottkw{nf}$} (5);
    \draw[->] (4) to (6);
    \draw[->] (5) to (8);
    \draw[->] (6) to node[left, draw=none]{$\ottkw{nf}$} (7);
  \end{tikzpicture}  
  \caption{Dependency graph of the subtyping algorithm}
  \label{fig:alg-subtyping-graph}
\end{figure}

In the remainder of this section, we will delve into each of these procedures in
detail, following the top-down order of the dependency graph. First, 
we present the subtyping algorithm itself.

As an input, the subtyping algorithm takes
a type context $[[Γ]]$, an instantiation context $[[Θ]]$,
and two types of the corresponding polarity:
$[[uN]]$ and  $[[iM]]$ for the negative subtyping, and
$[[uP]]$ and  $[[iQ]]$ for the positive subtyping.
We assume the second type ($[[iM]]$ and $[[iQ]]$) to be 
declarative (with no algorithmic variables) and well-formed in $[[Γ]]$,
but the first type ($[[uN]]$ and $[[uP]]$) may contain algorithmic variables,
whose instantiation contexts are specified by $[[Θ]]$.

Notice that the shape of the input types uniquely determines the
applied subtyping rule.  If the subtyping is successful, it returns
a set of constraints $[[SC]]$ restricting the algorithmic 
variables of the first type. If the subtyping does not hold, 
there will be no inference tree with such inputs. 

\begin{figure}[h]
  \hfill\\
  \begin{multicols}{2}
    \ottdefnANsubLabeled{}
    \columnbreak\\
    \ottdefnAPsupLabeled{}
  \end{multicols}
  \caption{Subtyping Algorithm}
  \label{fig:alg-subtyping}
\end{figure}

The rules of the subtyping algorithm bijectively correspond to the rules of the declarative
system. Let us discuss them in detail.

\paragraph*{Variables} Rules \ruleref{\ottdruleANVarLabel} and \ruleref{\ottdruleAPVarLabel}
say that if both of the input types are equal declarative variables,
they are subtypes of each other, with no constraints (as there are no algorithmic variables).

\paragraph*{Shifts} Rules \ruleref{\ottdruleAShiftDLabel} and
\ruleref{\ottdruleAShiftULabel} cover the downshift and the upshift cases,
respectively. If the input types are constructed by shifts, then the subtyping
can only hold if they are equivalent. This way, the algorithm must find the
instantiations of the algorithmic variables on the left-hand side
such that these instantiations make the left-hand side and the ride-hand side
equivalent. For this purpose, the algorithm invokes the
unification procedure (\cref{sec:unification}) preceded by the normalization of the input types.
It returns the resulting constraints given by the unification algorithm. 

\paragraph*{Quantifiers}  
Rules \ruleref{\ottdruleAForallLabel} and 
\ruleref{\ottdruleAExistsLabel} are symmetric. 
Declaratively, the quantified variables on the left-hand side must 
be instantiated with types, which are not known beforehand.
We address this problem by algorithmization 
of the quantified variables (see \cref{sec:algo-syntax}).
The rule introduces fresh algorithmic variables
$[[puas]]$ or $[[nuas]]$,
puts them into the instantiation context $[[Θ]]$
(specifying that they must be instantiated in the extended context
$[[Γ, pbs]]$ or $[[Γ, nbs]]$) and substitute the quantified variables
for them in the input type. 

After algorithmization of the quantified variables, 
the algorithm proceeds with the recursive call, returning
constraints $[[SC]]$. As the output, the algorithm removes the freshly
introduced algorithmic variables from the instantiation context, This operation is
sound: it is guaranteed that $[[SC]]$ always has a solution, but the specific
instantiation of the freshly introduced algorithmic variables is not important,
as they do not occur in the input types.

\paragraph*{Functions}
To infer the subtyping of the function types, the algorithm
makes two calls: 
\begin{enumerate*}
  \item[(i)] a recursive call ensuring the subtyping of the result types, and
  \item[(ii)] a call to positive subtyping (or rather super-typing) on the argument types.
\end{enumerate*}
The resulting constraints are merged
using a special procedure defined in \cref{sec:constraint-merge}
and returned as the output.

\paragraph*{Algorithmic variables}
If one of the sides of the subtyping is a unification variable, the algorithm
creates a new constraint. Because the right-hand side of the subtyping is always
declarative, it is only the left-hand side that can be a unification variable.
Moreover, another invariant we preserve prevents the negative algorithmic
variables from occurring in types during the negative subtyping algorithm. It
means that the only possible form of the subtyping here is $[[α̂⁺]] [[≥]]
[[iP]]$, which is covered by \ruleref{\ottdruleAPUVarLabel}.

The potential problem here is that the type $[[iP]]$ might be not well-formed in
the instantiation context required for $[[α̂⁺]]$ by $[[Θ]]$ because this
context might be smaller than the current context $[[Γ]]$. As we wish the
resulting constraint set to be sound \wrt $[[Θ]]$, we cannot simply put 
$[[α̂⁺ :≥ iP]]$ into the output. Prior to that, we update the type $[[iP]]$ to its
lowest supertype $[[iQ]]$ well-formed in $[[Θ(α̂⁺)]]$. It is done by the
\emph{upgrade} procedure, which we discuss in detail in \cref{sec:lub}.

\vspace{\baselineskip}
% \indent
To summarize, the subtyping algorithm uses the following additional subroutines:
\begin{enumerate*}[noitemsep]
  \item[(i)] rules \ruleref{\ottdruleAShiftDLabel} and
    \ruleref{\ottdruleAShiftULabel} invoke the \emph{unification} algorithm
    to equate the input types;
  \item[(ii)] rule \ruleref{\ottdruleAArrowLabel} \emph{merges} the constraints
    produced by the recursive calls on the result and the argument types; and
  \item[(iii)] rule \ruleref{\ottdruleAPUVarLabel} \emph{upgrades} the input type
    to its least supertype well-formed in the context required by the
    algorithmic variable.
\end{enumerate*}
The following sections discuss these additional procedures in detail.

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\subsection{Unification}
\label{sec:unification}

As an input, the unification context takes a type context $[[Γ]]$, an
instantiation context $[[Θ]]$, and two types of the required polarity: $[[uN]]$
and  $[[iM]]$ for the negative unification, and $[[uP]]$ and  $[[iQ]]$ for the
positive unification. It is assumed that only the left-hand side type may
contain algorithmic variables. This way, the left-hand side is well-formed as an
algorithmic type in $[[Γ]]$ and $[[Θ]]$, whereas the right-hand side is
well-formed declaratively in $[[Γ]]$.

Since only the left-hand side may contain algorithmic variables that the unification instantiates, we could have called this procedure \emph{matching}.
However, in \cref{sec:weakening-invariant}, we will discuss several
modifications of the type system, where this invariant is not preserved, and
therefore, this procedure requires general first-order pattern unification
\cite{miller1991:unification}.

As the output, the unification algorithm returns \emph{the weakest} set of
unification constraints $[[UC]]$ such that \emph{any} instantiation satisfying
these constraints unifies the input types.

\begin{figure}[h]
  \hfill
  \begin{multicols}{2}
  \ottdefnUNUnifLabeled{}
  \columnbreak\\
  \ottdefnUPUnifLabeled{}
  \end{multicols}
  \caption{Unification Algorithm}
  \label{fig:unification}
\end{figure}

The algorithm works as one might expect:
if both sides are formed by constructors, 
it is required that the constructors are the same, and the
types unify recursively. If one of the sides
is a unification variable (in our case it can only be the left-hand side),
we create a new unification constraint restricting it to be equal to the other side.
Let us discuss the rules that implement this strategy. 

\paragraph*{Variables}
  The variable rules \ruleref{\ottdruleUNVarLabel} and \ruleref{\ottdruleUPVarLabel}
  are trivial: as the input types do not have algorithmic variables, and are already equal, 
  the unification returns no constraints.

\paragraph*{Shifts}
  The shift rules \ruleref{\ottdruleUShiftDLabel} and \ruleref{\ottdruleUShiftULabel}
  require the two input types to be formed by the same shift constructor. 
  They remove this constructor, unify the types recursively, and return the resulting
  set of constraints.

\paragraph*{Quantifiers} 
  Similarly, the quantifier rules \ruleref{\ottdruleUForallLabel} and \ruleref{\ottdruleUExistsLabel}
  require the quantifier variables on the left-hand side and the right-hand side to be the same.
  This requirement is complete because we assume the input types of the unification 
  to be normalized, and thus, the equivalence implies alpha-equivalence. 
  In the implementation of this rule, an alpha-renaming might be needed to ensure 
  that the quantified variables are the same, however, we omit it for brevity.

\paragraph*{Functions}
  Rule \ruleref{\ottdruleUArrowLabel} unifies two functional types. 
  First, it unifies their argument types and their result types recursively.
  Then it merges the resulting constraints using the procedure described in \cref{sec:constraint-merge}.

  Notice that the resulting constraints can only have \emph{unification}
  entries. It means that they can be merged in a simpler way than general
  constraints. In particular, the merging procedure does not call any of the
  subtyping subroutines but rather simply checks the matching constraint entries
  for equality.

\paragraph*{Algorithmic variable}
  Finally, if the left-hand side of the unification is an algorithmic variable,
  \ruleref{\ottdruleUNVarLabel} or \ruleref{\ottdruleUPVarLabel} is applied. 
  It simply checks that the right-hand side type is well-formed in the required
  instantiation context, and returns a newly created constraint restricting the variable
  to be equal to the right-hand side type.

\vspace{\baselineskip}
As one can see, the unification procedure is standard; the only peculiarity
(although it is common for type inference) is that it makes sure that the resulting
instantiations agree with the input instantiation context $[[Θ]]$.  As a
subroutine, the unification algorithm only uses the (unification) constraint
merge procedure and the well-formedness checking.

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\subsection{Constraint Merge}
\label{sec:constraint-merge}

In this section, we discuss the constraint merging procedure.
It allows one to combine two constraint sets into one. 
A simple union of two constraint sets is not sufficient, 
since the resulting set must not contain two entries restricting the 
same algorithmic variable---we call such entries \emph{matching}.

The matching entries must be combined into \emph{one} constraint entry, that
would represent their conjunction. This way, to merge two constraint sets, we
unite the entries of two sets and then merge the matching pairs.

\paragraph*{Merging matching constraint entries}
Two \emph{matching} entries formed in the same context $[[Γ]]$ 
can be merged as shown in \cref{fig:merge-entries}.
Suppose that $[[scE1]]$ and $[[scE2]]$ are input entries. 
The result of the merge $[[scE1 & scE2]]$ is 
\emph{the weakest entry which implies both $[[scE1]]$ and $[[scE2]]$}.

\begin{figure}[h]
  \ottdefnSCMELabeled\\
  \caption{Merge of Matching Constraint Entries}
  \label{fig:merge-entries}
\end{figure}

Suppose that one of the input entries, say $[[scE1]]$, is a \emph{unification}
constraint entry. Then the resulting entry $[[scE1]]$ must coincide with it 
(up-to-equivalence), and thus, it is only required to check that $[[scE2]]$ 
is implied by $[[scE1]]$. We consider two options:
\begin{itemize}
  \item[(i)] if $[[scE2]]$ is also a \emph{unification} entry, then the types on
    the right-hand side of $[[scE1]]$ and $[[scE2]]$ must be equivalent, as
    given by rules \ruleref{\ottdruleSCMEPEqEqLabel} and
    \ruleref{\ottdruleSCMENEqEqLabel};
  \item[(ii)] if $[[scE2]]$ is a \emph{supertype} constraint entry $[[α̂⁺ :≥ iP]]$,
    the algorithm must check that the type assigned by $[[scE1]]$ is a supertype of $[[iP]]$.
    The corresponding symmetric rules are \ruleref{\ottdruleSCMESupEqLabel} and \ruleref{\ottdruleSCMEEqSupLabel}.
    In the premises of these rules, $[[uP]]$ is the same as $[[iP]]$ below, 
    and $[[uQ]]$ is the same as $[[iQ]]$ below but relaxed to an algorithmic type.
\end{itemize}

If both input entries are supertype constraints: $[[α̂⁺ :≥ iP]]$ and $[[α̂⁺ :≥ iQ]]$,
then their conjunction is $[[α̂⁺ :≥ iP ∨ iQ]]$, as given by \ruleref{\ottdruleSCMESupSupLabel}.
The least upper bound---$[[iP ∨ iQ]]$---is the least supertype of both $[[iP]]$ and $[[iQ]]$,
and this way, $[[α̂⁺ :≥ iP ∨ iQ]]$ is the weakest constraint entry that implies
$[[α̂⁺ :≥ iP]]$ and $[[α̂⁺ :≥ iQ]]$. The algorithm finding the least upper bound
is discussed in \cref{sec:lub}.

\paragraph*{Merging constraint sets}
  The algorithm for merging constraint sets is shown in \cref{fig:merge-subtyping-constraints}.
  As discussed, the result of merge $[[SC1]]$ and $[[SC2]]$ consists of three parts: 
  \begin{enumerate*}
    \item[(i)] the entries of $[[SC1]]$ that do not match any entry of $[[SC2]]$;
    \item[(ii)] the entries of $[[SC2]]$ that do not match any entry of $[[SC1]]$; and
    \item[(iii)] the merge (\cref{fig:merge-entries}) of matching entries.
  \end{enumerate*}


\begin{figure}[h]
  Suppose that $[[Θ ⊢ SC1]]$ and $[[Θ ⊢ SC2]]$.\\
  Then $[[Θ ⊢ SC1 & SC2 = SC]]$
  defines a set of constraints $[[SC]]$ such that $[[scE]] \in [[SC]]$ iff either:
  \begin{itemize}
    \item $[[scE]] \in [[SC1]]$ and there is no matching $[[scE']] \in [[SC2]]$; or
    \item $[[scE]] \in [[SC2]]$ and there is no matching $[[scE']] \in [[SC1]]$; or
    \item $[[Θ(α̂±) ⊢ scE1 & scE2 = scE]]$ for some $[[scE1]] \in [[SC1]]$ and $[[scE2]] \in [[SC2]]$
      such that $[[scE1]]$ and $[[scE2]]$ both restrict variable $[[α̂±]]$. 
  \end{itemize}

  \caption{Constraint Merge}
  \label{fig:merge-subtyping-constraints}
\end{figure}

As shown in \cref{fig:merge-entries}, the merging procedure relies 
substantially on the least upper bound algorithm.
In the next section, we discuss this algorithm in detail,
together with the upgrade procedure, selecting the least supertype 
well-formed \emph{in a given context}.

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\subsection{Type Upgrade and the Least Upper Bounds}
\label{sec:lub}

Both type upgrade and the least upper bound algorithms are used
to find a minimal supertype under certain conditions. 
For a given type $[[iP]]$ well-formed in $[[Γ]]$, the \emph{upgrade} operation 
finds the least among those supertypes of $[[iP]]$ that are well-formed
in a smaller context $[[Δ ⊆ Γ]]$.
For given two types $[[iP1]]$ and $[[iP2]]$ well-formed in $[[Γ]]$,
the \emph{least upper bound} operation finds the least among
common supertypes of $[[iP1]]$ and $[[iP2]]$ well-formed in $[[Γ]]$.
These algorithms are shown in \cref{fig:type-upgrade}.

\begin{figure}[h]
  \begin{multicols}{2}
    \ottdefnLUBUpLabeled{}
    \columnbreak\\
    \ottdefnLUBNsubLabeled{}
  \end{multicols}
  \caption{Type Upgrade and Leas Upper Bound Algorithms}
  \label{fig:type-upgrade}
\end{figure}

\paragraph*{The Type Upgarde}
The type upgrade algorithm uses the least upper bound algorithm as a subroutine.
It exploits the idea that the free variables of a positive type $[[iQ]]$
cannot disappear in its subtypes (see \cref{prop:subtyping-preserves-fv}). 
It means that if 
a type $[[iP]]$ has free variables not occurring 
in $[[iP']]$, then any common supertype of $[[iP]]$
and $[[iP']]$ must not contain these variables either.
This way, any supertype of $[[iP]]$
not containing certain variables $[[pnas]]$ must also be 
a supertype of $[[iP' = [pnbs/pnas]iP ]]$, where $[[pnbs]]$ are fresh;
and vice versa: any common supertype of $[[iP]]$ and $[[iP']]$
does not contain $[[pnas]]$ nor $[[pnbs]]$.

This way, to find the least supertype of $[[iP]]$ well-formed in $[[Δ]] = [[Γ \ {pnas}]]$
(\ie not containing $[[pnas]]$), we can do the following.
First, construct a new type $[[iP']]$ by renaming $[[pnas]]$ in $[[iP]]$ to fresh $[[pnbs]]$,
and second, find \emph{the least upper bound} of $[[iP]]$ and $[[iP']]$ in the appropriate
context. However, for reasons of symmetry, in rule
\ruleref{\ottdruleLUBUpgradeLabel} we employ a different but equivalent approach:
we create \emph{two} types $[[iP1]]$ and $[[iP2]]$ constructed by renaming $[[pnas]]$ in $[[iP]]$
to fresh disjoint variables $[[pnbs]]$ and $[[pncs]]$ respectively, and then 
find the least upper bound of $[[iP1]]$ and $[[iP2]]$.

\paragraph*{The Least Upper Bound}
The Least Upper Bound algorithm we use operates on \emph{positive}
types. This way, the inference rules of the algorithm
analyze the three possible shapes of the input types:
a variable type, an existential type, and a shifted computation.

 Rule \ruleref{\ottdruleLUBExistsLabel} covers the case when 
 at least one of the input types is an existential type.
 In this case, we can simply move the existential quantifiers
 from both sides to the context, and make a tail-recursive call.
 However, it is important to make sure that 
 the quantified variables $[[nas]]$ and $[[nbs]]$ are disjoint
 (\ie alpha-renaming might be required in the implementation).
  
 Rule \ruleref{\ottdruleLUBVarLabel} applies when 
 both sides are variables. In this case,
 the common supertype only exists if these variables are 
 the same. And if they are, the common supertypes
 must be equivalent to this variable.

Rule \ruleref{\ottdruleLUBShiftLabel} is the most 
interesting. If both sides are not quantified, and one of the sides is 
a shift, so must be the other side. 
However, the set of common upper bounds is not trivial in this case.
For example, $[[↓(β⁺ → γ1⁻)]]$ and $[[↓(β⁺ → γ2⁻)]]$ have
two non-equivalent common supertypes: 
$[[∃α⁻.↓α⁻]]$ 
(by instantiating $[[α⁻]]$ with $[[β⁺ → γ1⁻]]$ and $[[β⁺ → γ2⁻]]$ respectively)
and 
$[[∃α⁻.↓(β⁺ → α⁻)]]$ 
(by instantiating $[[α⁻]]$ with $[[γ1⁻]]$ and $[[γ2⁻]]$ respectively).
As one can see, the second supertype $[[∃α⁻.↓(β⁺ → α⁻)]]$ is the least among them
because it abstracts over a `deeper' negative subexpression.

In general, we must 
\begin{itemize*}
  \item[(i)] find the most detailed pattern (a type with `holes' at negative positions) 
    that matches both sides, and 
  \item[(ii)] abstract over the `holes' by existential quantifiers.
\end{itemize*}
The algorithm that finds the most detailed common pattern is called \emph{anti-unification}.
As output, it returns $[[(Ξ, uP, aus1, aus2)]]$, where important for us is
$[[uP]]$---the pattern---and $[[Ξ]]$--the set of `holes' represented by negative algorithmic variables.
We discuss the anti-unification algorithm in detail in the following section.


%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\subsection{Anti-Unification}
\label{sec:antiunification}

The anti-unification algorithm
\cite{plotkin1970:generalization,reynolds1970:transform}, is a procedure dual to
unification. For two given (potentially different) expressions, it finds the most
specific generalizer---the most detailed pattern that matches both of the input
expressions. As evidence, it can also return two substitutions that instantiate
the `holes' of the pattern to the input expressions.

In our case, we have to be more demanding on the anti-unification algorithm.
Since we use it to construct an existential type, whose (negative) quantified
variables can only be instantiated with negative types, we must make sure that
the pattern has `holes' only at negative positions. Moreover, we must make sure
that the resulting substitutions for the `holes' are well-formed in the context
from the past---at the moment when the corresponding polymorphic variables were
introduced---and do not contain variables bound later. For example, the
anti-unification of $[[iN1 = ∀β⁺.α1⁺ → ↑β⁺]]$ and $[[iN2 = ∀β⁺.α2⁺ → ↑β⁺]]$ is a
singleton `hole', which we model as an algorithmic type variable $[[γ̂⁻]]$, with
a pair of substitutions $[[γ̂⁻ ↦ iN1]]$ and $[[γ̂⁻ ↦ iN2]]$. But it
\emph{cannot} be more specific such as $[[∀β⁺.γ̂⁺ → ↑β⁺]]$ (since the hole
cannot be positive) or $[[∀β⁺.γ̂⁻]]$ (since \emph{the instantiation of $~[[γ̂⁻]]$ cannot capture the
bound variable $[[β⁺]]$}).

The algorithm that finds the most specific generalizer of two types
under required conditions is given in \cref{fig:anti-unification}.
It consists of two mutually recursive procedures:
the positive and the negative anti-unification. 
As the positive and the negative anti-unification procedures
are symmetric in their interface, let us discuss how to read
the positive judgment. 

The positive anti-unification judgment has form
$[[Γ ⊨ iP1 ≈au iP2 ⫤ ( Ξ , uQ , aus1 , aus2 )]]$.
As an input, it takes a context $[[Γ]]$, in which the `holes'
instantiations must be well-formed, 
and two positive types: $[[iP1]]$ and $[[iP2]]$;
it returns a tuple of four components:
$[[Ξ]]$---a set of `holes' represented by negative algorithmic variables,
$[[uQ]]$---a pattern represented as a positive algorithmic type,
whose algorithmic variables are in $[[Ξ]]$,
and two substitutions $[[aus1]]$ and $[[aus2]]$
instantiating the variables from $[[Ξ]]$ such that
$[[ [aus1]uQ = iP1 ]]$ and $[[ [aus2]uQ = iP2 ]]$. 

\begin{figure}[h]
    \ottdefnAUAUPLabeled{}
    \hfill\\
    \hfill\\
    \ottdefnAUAUNLabeled{}
    \caption{Anti-Unification Algorithm}
    \label{fig:anti-unification}
\end{figure}

At the high level, the algorithm scheme follows
the standard approach \cite{plotkin1970:generalization}
represented as a recursive procedure. Specifically, it follows two principles:
\begin{enumerate}
  \item[(i)] if the input terms start with the same constructor,
    we anti-unify the corresponding parts recursively and 
    unite the results. This principle is followed by 
    all the rules except \ruleref{\ottdruleAUAULabel},
    which works as follows:
  \item[(ii)] if the first principle does not apply to the input terms $[[iN]]$
    and $[[iM]]$ (for instance, if they have different outer constructors), the
    anti-unification algorithm returns a `hole' such that one substitution maps
    it to $[[iN]]$ and the other maps it to $[[iM]]$. This `hole'
    should have a name uniquely defined by the pair $([[iN]], [[iM]])$, so
    that it automatically merges with other `holes' mapped to the same
    pair of types, and thus, the initiality of the generalizer is ensured. 
\end{enumerate}

Let us discuss the specific rules of the algorithm in detail.

\paragraph*{Variables} 
  Rules \ruleref{\ottdruleAUPVarLabel} and
  \ruleref{\ottdruleAUNVarLabel} generalize two equal variables.
  In this case, the resulting pattern is the variable itself,
  and no `holes' are needed.

\paragraph*{Shifts} 
  Rules \ruleref{\ottdruleAUShiftDLabel} and
  \ruleref{\ottdruleAUShiftULabel} operate by congruence:
  they anti-unify the bodies of the shifts recursively and add
  the shift constructor back to the resulting pattern.

\paragraph*{Quantifiers}
  Rules \ruleref{\ottdruleAUForallLabel} and
  \ruleref{\ottdruleAUExistsLabel} are symmetric. 
  They generalize two quantified types congruently, 
  similarly to the shift rules. 
  However, we also require that the quantified variables
  are fresh, and that the left-hand side variables are 
  equal to the corresponding variables on the right-hand side.
  To ensure it, alpha-renaming might be required in the
  implementation.

  Notice that the context $[[Γ]]$ is \emph{not} extended with 
  the quantified variables. In this algorithm, $[[Γ]]$ 
  does not play the role of a current typing context, but rather
  a snapshot of a context at the moment of calling the anti-unification,
  \ie the context in which the instantiations of the `holes' 
  must be well-formed.

\paragraph*{Functions}
  Rule \ruleref{\ottdruleAUArrowLabel} congruently generalizes two function types. 
  An arrow type is the only binary constructor, 
  and thus, it is the only rule where the union of the anti-unification results is substantial.
  The interesting is the case when the resulting generalization of the 
  input types and the resulting generalization of the output types 
  have `holes' mapped to the same pair of types. 
  In this case, the algorithm must merge the `holes' into one.
  For example, the anti-unification of 
  $[[↓α⁻ → α⁻]]$ and $[[↓β⁻ → β⁻]]$
  must result in $[[↓γ̂⁻ → γ̂⁻]]$,
  rather than $[[↓γ1̂⁻ → γ2̂⁻]]$.

  In our representation of the anti-unification algorithm, this `merge' happens
  automatically:
  following the rule \ruleref{\ottdruleAUAULabel},
  the name of the `hole' is uniquely defined by the pair of types it is mapped to.  
  Specifically, when anti-unifying $[[↓α⁻ → α⁻]]$ and $[[↓β⁻ → β⁻]]$ our algorithm returns 
  $[[↓α̂⁻_{α⁻, β⁻} → α̂⁻_{α⁻, β⁻}]]$, that is a renaming of $[[↓γ̂⁻ → γ̂⁻]]$.

  This way, as the output the rule returns the following tuple:
  \begin{itemize}
    \item $[[Ξ1 ∪ Ξ2]]$---a simple union of the sets of `holes' 
      returned from by the recursive calls, 
    \item $[[uQ → uM]]$---the resulting pattern 
      constructed from the patterns returned recursively.
    \item $[[aus1 ∪ aus1']]$ and $[[aus2 ∪ aus'2]]$
      --- a union (in a relational sense)
      of the substitutions returned by the recursive calls. 
      It is worth noting that the union is well-defined because
      the result of the substation on a `hole' is determined by the 
      name of the `hole'.
  \end{itemize}

\paragraph*{The Anti-Unification Rule}
  Rule \ruleref{\ottdruleAUAULabel} is the base case of the anti-unification
  algorithm. If the congruent rules are not applicable, 
  it means that the input types have a substantially different structure,
  and thus, the only option is to create a `hole'. 
  There are three important aspects of this rule that we would like to discuss.

  First, as mentioned earlier, the freshly created `hole' has a name that is
  uniquely defined by the pair of input types. It is ensured by the following
  invariant: all the `holes' in the algorithm have name $[[α̂⁻]]$ indexed by the
  pair of negative types it is mapped to. This way, the returning set of `holes'
  is a singleton set $\{ [[ â⁻_{iN, iM} ]]\}$; the resulting pattern is the
  `hole' $[[â⁻_{iN, iM}]]$, and the mappings simply send it to the
  corresponding types: $[[â⁻_{iN, iM} ↦ iN]]$ and $[[â⁻_{iN, iM} ↦ iM]]$.

  Second, this rule is only applicable to negative types; moreover, the input
  types are checked to be well-formed in the outer context $[[Γ]]$. This is
  required by the usage of anti-unification: we call it to build an existential
  type that would be an upper bound of two input types via abstracting some of
  their subexpressions under existential quantifiers. The existentials quantify
  over \emph{negative} variables, and they must be instantiated in the context
  available at that moment.

  Third, the rule is only applicable if all other rules fail. Notice that it
  could happen even when the input types have matching constructors. For
  example, the generalizer of $[[↑α⁺]]$ and $[[↑β⁺]]$ is $[[γ̂⁻]]$ (with
  mappings $[[γ̂⁻ ↦ ↑α⁺]]$ and $[[γ̂⁻ ↦ ↑β⁺]]$), rather than $[[↑γ̂⁺]]$. This
  way, the algorithm must try to apply the congruent rules first, and only if
  they fail, apply \ruleref{\ottdruleAUAULabel}. As it is not known a priori
  whether the congruent rules will be applicable, the implementation must use 
  backtracking.

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\subsection{Type Inference}
\label{sec:typing}

Finally, we present the type inference algorithm. Similarly to the subtyping
algorithm, it structurally corresponds to the declarative inference
specification, meaning that most of the algorithmic rules have declarative
counterparts, with respect to which they are sound and complete.

This way, the inference algorithm also consists of three mutually recursive
procedures: the positive type inference, the negative type inference, and the
application type inference. As subroutines, the inference algorithm uses
subtyping, constraint merge, and minimal instantiation. The corresponding
dependency is shown in \cref{fig:alg-typing-graph}.

\begin{figure}
  \centering
  \begin{tikzpicture}
    [>={Stealth[scale=2]},node distance=2.2cm,every node/.style={draw,rectangle},every text node part/.style={align=center}]
    % Define nodes
    \node[] (1) {Positive Inference\\$[[Γ; Φ ⊨ v : iP]]$};
    \node[right=1.4cm of 1] (2) {Negative Inference\\$[[Γ; Φ ⊨ c : iN]]$};
    \node[right=1.2cm of 2] (3) {Application Inference\\$[[Γ ; Φ ; Θ1 ⊨ uN ● args ⇒> uM ⫤ Θ2 ; SC]]$};
    \node[below=0.6cm of 2] (7) {Minimal Instantiation\\$[[Γ ⊢ uP SC minby uσ]]$\\(\cref{sec:constraint-singularity})};
    \node[below=0.5cm of 7] (8) {Constraint Singularity\\$[[SC singular with uσ]]$\\(\cref{sec:constraint-singularity})};
    \node[right=0.3cm of 7] (4) {Constraint Merge\\$[[Θ ⊢ SC1 & SC2 = SC3]]$\\(\cref{sec:constraint-merge})};
    \node[left=0.3cm of 7] (6) {Negative Subtyping\\$[[Γ ; Θ ⊨ uN1 ≤ iN2 ⫤ SC]]$\\(\cref{sec:subtyping-algorithm})};
    \node[below=0.9cm of 6] (5) {Positive Subtyping\\$[[Γ ; Θ ⊨ uP1 ≥ iP2 ⫤ SC]]$\\(\cref{sec:subtyping-algorithm})};
    \draw[->] ([yshift=-.2cm]2.west) to ([yshift=-.2cm]1.east);
    \draw[->] ([yshift=+.2cm]1.east) to ([yshift=+.2cm]2.west);
    \draw[->, bend right=20] (3) to (1);
    \draw[->] (2) to (3);
    \draw[->] (2) to (4);
    \draw[->] (2) to (7);
    \draw[->] (3) to (4);
    \draw[->] (7) to (8);
    \draw[->] (1.south west) |- (5);
    \draw[->] (3) |- (5.south east);
    \draw[->] (2) to (6);
  \end{tikzpicture}  
  \caption{Dependency graph of the typing algorithm}
  \label{fig:alg-typing-graph}
\end{figure}

The positive and the negative type inference judgments have symmetric forms:
$[[Γ; Φ ⊨ v : iP]]$ and $[[Γ; Φ ⊨ c : iN]]$. Both of these algorithms
take as an input typing context $[[Γ]]$, a variable context $[[Φ]]$, and 
a term (a value or a computation) taking its type variables from $[[Γ]]$, 
and term variables from $[[Φ]]$. As an output, they return a type
of the given term, which we guarantee to be normalized. 

The application type inference judgment has form 
$[[Γ ; Φ ; Θ1 ⊨ uN ● args ⇒> uM ⫤ Θ2 ; SC]]$.
As an input, it takes three contexts: typing context $[[Γ]]$, a variable context $[[Φ]]$,
and an instantiation context $[[Θ1]]$. It also takes a head type $[[uN]]$ and 
a list of arguments (terms) $[[args]]$ the head is applied to.
The head may contain algorithmic variables specified by $[[Θ1]]$, 
in other words, $[[Γ; dom(Θ1) ⊢ uN]]$.
As a result, the application inference judgment returns 
$[[uM]]$---a normalized type of the result of the application.
Type $[[uM]]$ may contain new algorithmic variables, and thus, 
the judgment also returns $[[Θ2]]$---an updated instantiation context
and $[[SC]]$---a set of subtyping constraints.
Together $[[Θ2]]$  and $[[SC]]$ specify how the algorithmic variables 
must be instantiated.


\begin{figure}
  \ottdefnATPInfLabeled{}
  \hfill \\
  \ottdefnATNInfLabeled{}
  \hfill \\
  \ottdefnATSpinInfLabeled{}
  \caption{Algorithmic Type Inference}
  \label{fig:type-inference}
\end{figure}


The inference rules are shown in
\cref{fig:type-inference}.
Next, we discuss them in detail.

\paragraph*{Variables}
  Rule \ruleref{\ottdruleATVarLabel} 
  infers the type of a positive variable by looking it up in the 
  term variable context and normalizing the result.

\paragraph*{Annotations}
  Rules \ruleref{\ottdruleATPAnnotLabel} and \ruleref{\ottdruleATNAnnotLabel}
  are symmetric.
  First, they check that the annotated type is well-formed in the 
  given context $[[Γ]]$. Then they make a recursive call to infer the 
  type of annotated expression, check that the inferred type is a subtype of 
  the annotation, and return the normalized annotation.

\paragraph*{Abstractions}
  Rule \ruleref{\ottdruleATtLamLabel} infers the type of a lambda abstraction.
  It checks the well-formedness of the annotation $[[iP]]$,
  makes a recursive call to infer the type of the body in the extended context, 
  and returns the corresponding arrow type.
  Since the annotation $[[iP]]$ is allowed to be non-normalized,
  the rule also normalizes the resulting type.

  Rule \ruleref{\ottdruleATTLamLabel} infers the type of a big lambda.
  Similarly to the previous case, it makes a recursive call to infer the type
  of the body in the extended \emph{type} context. 
  After that, it returns the corresponding universal type. 
  It is also required to normalize the result.  
  For instance, if $[[α⁺]]$ does not occur in the body of the lambda,
  the corresponding $[[∀]]$ will be removed.

\paragraph*{Return and Thunk}
  Rules \ruleref{\ottdruleATThunkLabel} and \ruleref{\ottdruleATReturnLabel}
  are similar to the declarative rules: they make a recursive call
  to type the body of the thunk or the return expression and
  put the shift on top of the result.

\paragraph*{Unpack}
  Rule \ruleref{\ottdruleATUnpackLabel}
  allows one to unpack an existential type.
  First, it infers the existential type $[[∃nas.iP]]$ of the value being unpacked,
  and since the type is guaranteed to be normalized, binds 
  the quantified variables with $[[nas]]$.
  Then it infers the type of the body in the appropriately extended context
  and checks that the inferred type does not depend on $[[nas]]$
  by checking well-formedness $[[Γ ⊢ iN]]$.

\paragraph*{Let Binders}
  Rule \ruleref{\ottdruleATVarLetLabel} represents the type inference of a
  standard let binder. It infers the type of the bound value $[[v]]$, and makes
  a recursive call to infer the type of the body in the extended context.

  Rule \ruleref{\ottdruleATCVarLetLabel} infers a type of computational let
  binder. It follows the corresponding declarative rule
  \ruleref{\ottdruleDTCVarLetLabel} but uses algorithmic judgments instead of
  declarative ones. It is worth noting that when calling the subtyping 
  $[[Γ ; · ⊨ uM ≤ ↑iP ⫤ ·]]$, both $[[uM]]$ and $[[iP]]$ are free of algorithmic
  variables: $[[uM]]$ is a type inferred for $[[c]]$, and $[[iP]]$ is given as
  an annotation.

  Rule \ruleref{\ottdruleATAppLetAnnLabel}
  infers a type of \emph{annotated} applicative let binder.
  First, it infers the type of the head of the application,
  ensuring that it is a \emph{thunked computation} $[[↓iM]]$.
  After that, it makes a recursive call
  to the application inference procedure,
  returning an algorithmic type $[[uM']]$, 
  that must be a subtype of the annotation $[[↑iP]]$.

  Then premise $[[Γ; Θ ⊨ uM' ≤ ↑iP ⫤ SC2]]$
  together with $[[Θ ⊢ SC1 & SC2 = SC]]$
  check that $[[uM']]$ can be instantiated to the annotated type $[[↑iP]]$,
  and if it is, the algorithm infers the type of the body in the extended context,
  and returns it as the result. 

  Rule \ruleref{\ottdruleATAppLetLabel}
  works similarly to \ruleref{\ottdruleATAppLetAnnLabel}.
  However, since no annotation is given,
  the algorithm must ensure that the inferred $[[uQ]]$
  has the `canonical' minimal instantiation.
  To find it, it makes a call to the minimal instantiation algorithm 
  (\cref{sec:constraint-singularity})
  that finds the substitution that satisfies the inferred constraints $[[SC]]$ and
  instantiates $[[uQ]]$ to the minimal (among other such instantiations)
  type $[[ [uσ]uQ ]]$.


\paragraph*{Application to an Empty List of Arguments}
  Rule \ruleref{\ottdruleATEmptyAppLabel}
  is the base case of application inference. 
  If the list of applied arguments is empty, 
  the inferred type is the type of the head,
  and the algorithm returns it after normalizing.

\paragraph*{Application of a Polymorphic Type $[[∀]]$}
  Rule \ruleref{\ottdruleATForallAppLabel},
  analogously to the declarative case,
  is the rule ensuring the implicit elimination of the universal quantifiers. 
  This is the place where the algorithmic variables are introduced.
  The algorithm simply replaces the quantified variables 
  $[[pas]]$ with fresh algorithmic variables $[[puas]]$,
  and makes a recursive call in the extended context.

  To ensure the rule precedence, we also require
  the head type to have at least one $[[∀]]$-quantifier, 
  and the list of arguments to be non-empty.

\paragraph*{Application of an Arrow Type}
  Rule \ruleref{\ottdruleATArrowAppLabel}
  is the main rule of algorithmic application inference.
  It is applied when the head has an arrow type $[[uQ → uN]]$.
  First, it infers the type of the first argument $[[v]]$,
  and then, calling the algorithmic subtyping,
  finds $[[SC1]]$---the \emph{minimal} constraint ensuring that 
  $[[uQ]]$ is a supertype of the type of $[[v]]$.
  Then it makes a recursive call applying $[[uN]]$ to the rest of the arguments 
  and merges the resulting constraint with $[[SC1]]$.

\subsection{Minimal Instantiation and Constraint Singularity}
\label{sec:constraint-singularity}

Multiple types $[[iM]]$ can be inferred for a type application:
$[[Γ ; Φ ⊢ iN ● args ⇒> iM]]$ but only one \emph{principal} 
type should be chosen for a variable in an unannotated let binder.
Declaratively, we require the principal type $[[iP]]$ to be
minimal among other all types $[[iP']]$ that can be inferred
for the application $[[Γ ; Φ ⊢ iN ● args ⇒> ↑iP']]$.
Algorithmically, the inference returns an
\emph{algorithmic} type $[[uP]]$ 
together with a set of \emph{necessary and sufficient} 
constraints $[[SC]]$ that any instantiation of $[[uP]]$ must satisfy.
This way, the principal type is the minimal instantiation of $[[uP]]$,
obtained by a substitution satisfying the given constraints $[[SC]]$. 

To find this substitution, we use the minimal instantiation
algorithm (\cref{fig:minimal-instantiation}). The judgment `$[[Γ ⊢ uP SC minby uσ]]$'
$[[uσ]]$ instantiates $[[uP]]$ to a subtype of any other 
instantiation of $[[uP]]$ that satisfies $[[SC]]$. 
First, it removes the existential quantifiers by \ruleref{\ottdruleSINGExistsLabel}.
Then it considers two cases. 
\begin{enumerate}
  \item If the type $[[uP]]$ is an algorithmic variable $[[α̂⁺]]$
    restricted by a \emph{subtyping} constraint $[[(α̂⁺ :≥ iQ) ∊ SC]]$,
    its minimal instantiation is the type $[[nf(iQ)]]$, and 
    \ruleref{\ottdruleSINGPUvarLabel} is applied.
  \item Otherwise, the type $[[uP]]$ is either a shifted computation, a
    declarative variable, an unrestricted algorithmic variable, or an
    algorithmic variable restricted by an \emph{equivalence} constraint. In all
    of these cases, the minimal instantiation exists if and only if there is
    only one possible instantiation of $[[uP]]$ that satisfies $[[SC]]$.
    
    The algorithm finds this instantiation by two premises:
    \begin{enumerate*}
      \item[(i)] checking that all the algorithmic variables of the type are
      restricted by the constraint set; and
      \item[(ii)] building the substitution that satisfies the constraint set on
        these variables (and simultaneously checking that such substitution is
        unique up to equivalence) using the \emph{constraint singularity algorithm}.
    \end{enumerate*}
  \end{enumerate}


\begin{figure}[h]
  \hfill\\
  \ottdefnMININSTLabeled
  \caption{Minimal Instantiation}
  \label{fig:minimal-instantiation}
\end{figure}

The singularity algorithm 
performs two tasks: it checks that a
\emph{constraint set} has a single substitution satisfying it, 
and if it does, it builds this substitution.

\begin{figure}
  \begin{minipage}{0.49\textwidth}
    `$[[SC singular with uσ]]$' means:
    \begin{itemize}[leftmargin=*]
      \item[$+$] for any \emph{positive} constraint entry $[[scE ∊ SC]]$ restricting 
        a variable $[[β̂⁺]]$, there exists $[[iP]]$ such that $[[scE singular with iP]]$
        (as defined in \cref{fig:constraint-entry-singularity}),
        and $[[ [uσ]β̂⁺ = iP ]]$; and
      \item[$-$] the symmetric property holds for all \emph{negative} $[[scE ∊ SC]]$; and
      \item[$\notin$] for any $[[α̂± ∉ dom(SC)]]$, $[[ [uσ]α̂± = α̂± ]]$.
    \end{itemize}
    \caption{Singular Constraint}
    \label{fig:constraint-singularity}
  \end{minipage}
  \begin{minipage}{0.48\textwidth}
    \ottdefnSINGLabeled
    \caption{Singular Constraint Entry}
    \label{fig:constraint-entry-singularity}
  \end{minipage}
\end{figure}

To implement the singularity algorithm, we define a partial function 
`$[[SC singular with uσ]]$', taking a subtyping constraint $[[SC]]$ as an argument and
returning a substitution $[[uσ]]$---the only solution of $[[SC]]$. 

The constraint $[[SC]]$ is composed of constraint entries. Therefore, we define
singularity by combining the singularity of each constraint entry
(\cref{fig:constraint-singularity}). In order for $[[SC]]$ to be singular, each
entry must have a unique instantiation, and the resulting substitution $[[uσ]]$
must be a union of these instantiations. In addition, $[[uσ]]$ must act as an
identity on the variables not restricted by $[[SC]]$.

The singularity of constraint \emph{entries} is defined in
\cref{fig:constraint-entry-singularity}. 
\begin{itemize}
  \item The \emph{equivalence} entries are
    always singular, as the only possible type satisfying them is the one given in
    the constraint itself, which is reflected in rules \ruleref{\ottdruleSINGNEqLabel} and
    \ruleref{\ottdruleSINGPEqLabel}. 
  \item The \emph{subtyping} constraints are trickier.
    As will be discussed in \cref{sec:proof-lub-upgrade}, variables (and equivalent
    them $[[∃nas.pa]]$) do not have proper supertypes, and thus, the
    constraints of a form $[[pua :≥ ∃nas.pa]]$ are singular with the only possible
    normalized solution $[[pa]]$---see rule \ruleref{\ottdruleSINGSupVarLabel}. 
  \item However, if the body of the existential type is guarded by a shift
    $[[∃nas.↓iN]]$, it is singular if and only if $[[iN]]$ is equivalent to some
    $[[αi⁻ ∊ {nas}]]$ bound by the quantifier---see rule
    \ruleref{\ottdruleSINGSupShiftLabel}. The completeness of this criterion is
    justified by the fact that if $[[iN]]$ is \emph{not} equivalent to any
    $[[αi⁻]]$, then there are two non-equivalent solutions of constraint $[[(pua :≥
    ∃nas.↓iN)]]$: the trivial $[[∃nas.↓iN]]$ and $[[∃nas.↓a1⁻]]$ (which is a
    supertype of $[[∃nas.↓iN]]$ since $[[a1⁻]]$ can be instantiated to
    $[[iN]]$).

\end{itemize}

